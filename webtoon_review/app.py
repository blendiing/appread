import streamlit as st
from google_play_scraper import reviews, Sort
import pandas as pd
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import re

# ----------------------------
# í˜ì´ì§€ ì„¤ì •
# ----------------------------
st.set_page_config(
    page_title="ë„¤ì´ë²„ ì›¹íˆ° ì•±ë¦¬ë·° ë¶„ì„",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ----------------------------
# í•œê¸€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì € (KoNLPy ëŒ€ì²´)
# Streamlit Cloudì—ì„œ Java ì„¤ì¹˜ê°€ ì–´ë ¤ìš°ë¯€ë¡œ ì •ê·œì‹ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
# ----------------------------

# ë¶ˆìš©ì–´ ì •ì˜
STOPWORDS = {
    "ë„ˆë¬´", "ì •ë§", "ì§„ì§œ", "ë§¤ìš°", "ì•„ì£¼", "ì™„ì „", "ë˜ê²Œ", "ê½¤", "ì¢€", "ì•½ê°„", "ì‚´ì§",
    "ì›¹íˆ°", "ê·¸ëƒ¥", "ì´ê±°", "ì €ê±°", "ê·¸ê²ƒ", "ì´ê²ƒ", "ì €ê²ƒ", "í•˜ëŠ”", "ìˆëŠ”", "ì—†ëŠ”",
    "í•´ì„œ", "í•˜ê³ ", "í•´ìš”", "í•©ë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "ìˆì–´ìš”", "ì—†ì–´ìš”", "ê°™ì•„ìš”",
    "ì´ëŸ°", "ì €ëŸ°", "ê·¸ëŸ°", "ì–´ë–¤", "ë¬´ìŠ¨", "ì™œ", "ì–´ë””", "ì–¸ì œ", "ì–´ë–»ê²Œ",
    "ê·¼ë°", "ê·¸ë˜ì„œ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ê·¸ë˜ë„",
    "ì•±", "ì–´í”Œ", "ì•±ì´", "ì–´í”Œì´", "ë„¤ì´ë²„", "naver"
}

def simple_tokenizer(text):
    """ì •ê·œì‹ ê¸°ë°˜ í•œê¸€ í† í¬ë‚˜ì´ì €"""
    # í•œê¸€ 2ê¸€ì ì´ìƒ ë‹¨ì–´ ì¶”ì¶œ
    tokens = re.findall(r"[ê°€-í£]{2,}", str(text))
    # ë¶ˆìš©ì–´ ì œê±°
    tokens = [t for t in tokens if t not in STOPWORDS and len(t) >= 2]
    return tokens

@st.cache_data(ttl=3600, show_spinner=False)
def get_reviews(app_id, count=1000):
    """Google Play ë¦¬ë·° ìˆ˜ì§‘ (ìºì‹± ì ìš©)"""
    result = []
    continuation_token = None
    
    while len(result) < count:
        batch_size = min(200, count - len(result))
        review_batch, continuation_token = reviews(
            app_id,
            lang="ko",
            country="kr",
            sort=Sort.NEWEST,
            count=batch_size,
            continuation_token=continuation_token
        )
        result.extend(review_batch)
        
        if not continuation_token:
            break
    
    df = pd.DataFrame(result)
    if not df.empty:
        df["at"] = pd.to_datetime(df["at"])
        df["content"] = df["content"].astype(str)
    return df

def extract_keywords(df):
    """í‚¤ì›Œë“œ ì¶”ì¶œ"""
    tokens = []
    for text in df["content"]:
        tokens += simple_tokenizer(text)
    return tokens

# ----------------------------
# ë©”ì¸ UI
# ----------------------------
st.title("ğŸ“Š ë„¤ì´ë²„ ì›¹íˆ° ì•± ë¦¬ë·° ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
st.markdown("Google Play Store ë¦¬ë·°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    app_id = st.text_input(
        "ì•± ID",
        value="com.nhn.android.webtoon",
        help="Google Play Store ì•± IDë¥¼ ì…ë ¥í•˜ì„¸ìš”"
    )
    
    review_count = st.slider(
        "ìˆ˜ì§‘í•  ë¦¬ë·° ìˆ˜",
        min_value=100,
        max_value=3000,
        value=1000,
        step=100
    )
    
    analyze_btn = st.button("ğŸ” ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True)
    
    st.markdown("---")
    st.markdown("### ğŸ“Œ ë‹¤ë¥¸ ì•± ë¶„ì„ ì˜ˆì‹œ")
    st.code("com.kakaopage.app", language=None)
    st.code("com.lezhin.comics", language=None)
    st.code("com.toptoon.app", language=None)

# ----------------------------
# ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
# ----------------------------
if analyze_btn or "df" in st.session_state:
    
    if analyze_btn:
        with st.spinner(f"ğŸ“¥ ë¦¬ë·° ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... (ìµœëŒ€ {review_count}ê±´)"):
            df = get_reviews(app_id, review_count)
            df = df.sort_values(by="at", ascending=False)
            st.session_state["df"] = df
            st.session_state["app_id"] = app_id
    else:
        df = st.session_state["df"]
        app_id = st.session_state.get("app_id", app_id)
    
    if df.empty:
        st.error("ë¦¬ë·° ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•± IDë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        st.success(f"âœ… ì´ **{len(df):,}ê±´**ì˜ ë¦¬ë·° ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. (ì•± ID: `{app_id}`)")
        
        # íƒ­ êµ¬ì„±
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ˆ í†µê³„", "ğŸ’¬ í‚¤ì›Œë“œ", "ğŸ”— ì—°ê´€ì–´", "ğŸ“ ë¦¬ë·° ì›ë¬¸"])
        
        # ----------------------------
        # íƒ­ 1: ê¸°ë³¸ í†µê³„
        # ----------------------------
        with tab1:
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ì´ ë¦¬ë·° ìˆ˜", f"{len(df):,}ê±´")
            with col2:
                avg_score = df["score"].mean()
                st.metric("í‰ê·  í‰ì ", f"{avg_score:.2f}â­")
            with col3:
                recent_week = df[df["at"] >= df["at"].max() - pd.Timedelta(days=7)]
                st.metric("ìµœê·¼ 7ì¼ ë¦¬ë·°", f"{len(recent_week):,}ê±´")
            with col4:
                five_star_ratio = (df["score"] == 5).sum() / len(df) * 100
                st.metric("5ì  ë¹„ìœ¨", f"{five_star_ratio:.1f}%")
            
            st.markdown("---")
            
            # ë‚ ì§œë³„ ë¦¬ë·° ìˆ˜ ì¶”ì´
            st.subheader("ğŸ—“ï¸ ë‚ ì§œë³„ ë¦¬ë·° ìˆ˜ ì¶”ì´")
            daily_counts = df.groupby(df["at"].dt.date).size().reset_index(name="ë¦¬ë·°ìˆ˜")
            daily_counts.columns = ["ë‚ ì§œ", "ë¦¬ë·°ìˆ˜"]
            st.line_chart(daily_counts.set_index("ë‚ ì§œ"))
            
            # í‰ì  ë¶„í¬
            st.subheader("â­ í‰ì  ë¶„í¬")
            score_counts = df["score"].value_counts().sort_index()
            st.bar_chart(score_counts)
        
        # ----------------------------
        # íƒ­ 2: í‚¤ì›Œë“œ ë¶„ì„
        # ----------------------------
        with tab2:
            st.subheader("ğŸ’¬ ì£¼ìš” í‚¤ì›Œë“œ TOP 30")
            
            tokens = extract_keywords(df)
            counter = Counter(tokens)
            common_words = counter.most_common(30)
            
            if common_words:
                # ì›Œë“œí´ë¼ìš°ë“œ
                try:
                    # ì‹œìŠ¤í…œ í°íŠ¸ ì°¾ê¸°
                    import matplotlib.font_manager as fm
                    font_path = None
                    
                    # ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ ê²½ë¡œë“¤
                    possible_fonts = [
                        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
                        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
                        "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
                    ]
                    
                    for fp in possible_fonts:
                        try:
                            if fm.FontProperties(fname=fp):
                                font_path = fp
                                break
                        except:
                            continue
                    
                    wordcloud = WordCloud(
                        font_path=font_path,
                        width=800,
                        height=400,
                        background_color="white",
                        colormap="viridis"
                    ).generate_from_frequencies(dict(common_words))
                    
                    fig, ax = plt.subplots(figsize=(10, 5))
                    ax.imshow(wordcloud, interpolation="bilinear")
                    ax.axis("off")
                    st.pyplot(fig)
                    plt.close()
                except Exception as e:
                    st.warning(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                
                # í‚¤ì›Œë“œ í…Œì´ë¸”
                keyword_df = pd.DataFrame(common_words, columns=["í‚¤ì›Œë“œ", "ë¹ˆë„"])
                
                col1, col2 = st.columns([1, 1])
                with col1:
                    st.dataframe(keyword_df.head(15), use_container_width=True)
                with col2:
                    st.dataframe(keyword_df.tail(15), use_container_width=True)
            else:
                st.info("ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ----------------------------
        # íƒ­ 3: ì—°ê´€ì–´ ë¶„ì„
        # ----------------------------
        with tab3:
            st.subheader("ğŸ”— ì£¼ìš” í‚¤ì›Œë“œ ì—°ê´€ ë‹¨ì–´")
            
            co_occurrence = {}
            for text in df["content"]:
                tokens = simple_tokenizer(text)
                for i in range(len(tokens) - 1):
                    a, b = tokens[i], tokens[i + 1]
                    if a != b:
                        co_occurrence.setdefault(a, []).append(b)
            
            related_words = []
            for k, v in counter.most_common(30):
                related = Counter(co_occurrence.get(k, [])).most_common(5)
                related_words.append({
                    "í‚¤ì›Œë“œ": k,
                    "ë¹ˆë„": v,
                    "ì—°ê´€ë‹¨ì–´": ", ".join([f"{r[0]}({r[1]})" for r in related]) if related else "-"
                })
            
            related_df = pd.DataFrame(related_words)
            st.dataframe(related_df, use_container_width=True)
        
        # ----------------------------
        # íƒ­ 4: ë¦¬ë·° ì›ë¬¸
        # ----------------------------
        with tab4:
            st.subheader("ğŸ“ ìµœì‹  ë¦¬ë·° ì›ë¬¸")
            
            # í•„í„°ë§ ì˜µì…˜
            col1, col2 = st.columns(2)
            with col1:
                score_filter = st.multiselect(
                    "í‰ì  í•„í„°",
                    options=[1, 2, 3, 4, 5],
                    default=[1, 2, 3, 4, 5]
                )
            with col2:
                search_keyword = st.text_input("í‚¤ì›Œë“œ ê²€ìƒ‰", "")
            
            filtered_df = df[df["score"].isin(score_filter)]
            
            if search_keyword:
                filtered_df = filtered_df[
                    filtered_df["content"].str.contains(search_keyword, case=False, na=False)
                ]
            
            st.write(f"**{len(filtered_df):,}ê±´**ì˜ ë¦¬ë·°ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # í˜ì´ì§€ë„¤ì´ì…˜
            page_size = 20
            total_pages = (len(filtered_df) - 1) // page_size + 1
            page = st.number_input("í˜ì´ì§€", min_value=1, max_value=max(1, total_pages), value=1)
            
            start_idx = (page - 1) * page_size
            end_idx = start_idx + page_size
            
            for _, row in filtered_df.iloc[start_idx:end_idx].iterrows():
                with st.container():
                    col1, col2 = st.columns([1, 5])
                    with col1:
                        st.write(f"â­ **{row['score']}ì **")
                        st.caption(str(row['at'].date()))
                    with col2:
                        st.write(row["content"])
                    st.markdown("---")

else:
    # ì´ˆê¸° í™”ë©´
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ **ë¶„ì„ ì‹œì‘** ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”!")
    
    st.markdown("""
    ### ğŸ¯ ì´ ëŒ€ì‹œë³´ë“œë¡œ í•  ìˆ˜ ìˆëŠ” ê²ƒ
    
    - **ì‹¤ì‹œê°„ ë¦¬ë·° ìˆ˜ì§‘**: Google Play Storeì—ì„œ ìµœì‹  ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤
    - **í‚¤ì›Œë“œ ë¶„ì„**: ë¦¬ë·°ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤
    - **ì—°ê´€ì–´ ë¶„ì„**: í‚¤ì›Œë“œ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤
    - **íŠ¸ë Œë“œ íŒŒì•…**: ë‚ ì§œë³„ ë¦¬ë·° ì¶”ì´ì™€ í‰ì  ë¶„í¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤
    
    ### ğŸš€ ì‹œì‘í•˜ê¸°
    
    1. ì‚¬ì´ë“œë°”ì—ì„œ ì•± IDë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸: ë„¤ì´ë²„ ì›¹íˆ°)
    2. ìˆ˜ì§‘í•  ë¦¬ë·° ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”
    3. **ë¶„ì„ ì‹œì‘** ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”!
    """)

# ----------------------------
# í‘¸í„°
# ----------------------------
st.markdown("---")
st.caption("Made with â¤ï¸ using Streamlit | ë°ì´í„° ì¶œì²˜: Google Play Store")

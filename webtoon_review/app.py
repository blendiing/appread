import streamlit as st
from google_play_scraper import reviews, Sort
import pandas as pd
from collections import Counter
from wordcloud import WordCloud
import re
import os
from io import BytesIO

# ----------------------------
# í˜ì´ì§€ ì„¤ì •
# ----------------------------
st.set_page_config(
    page_title="ê²½ìŸì‚¬ ì•± ë¦¬ë·° ë¶„ì„",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ----------------------------
# í°íŠ¸ ê²½ë¡œ ì„¤ì •
# ----------------------------
def get_font_path():
    possible_paths = [
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf",
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
    ]
    for path in possible_paths:
        if os.path.exists(path):
            return path
    return None

FONT_PATH = get_font_path()

# ----------------------------
# ì•± ëª©ë¡ ì •ì˜
# ----------------------------
APP_LIST = {
    "ë„¤ì´ë²„ ì›¹íˆ°": "com.nhn.android.webtoon",
    "ì¹´ì¹´ì˜¤í˜ì´ì§€": "com.kakaopage.app",
    "ë ˆì§„ì½”ë¯¹ìŠ¤": "com.lezhin.comics",
    "ë¦¬ë””ë¶ìŠ¤": "com.initialcoms.ridi",
}

# ----------------------------
# ë¶ˆìš©ì–´ ì •ì˜
# ----------------------------
STOPWORDS = {
    "ë„ˆë¬´", "ì •ë§", "ì§„ì§œ", "ë§¤ìš°", "ì•„ì£¼", "ì™„ì „", "ë˜ê²Œ", "ê½¤", "ì¢€", "ì•½ê°„", "ì‚´ì§",
    "ê·¸ëƒ¥", "ì´ê±°", "ì €ê±°", "ê·¸ê²ƒ", "ì´ê²ƒ", "ì €ê²ƒ", "í•˜ëŠ”", "ìˆëŠ”", "ì—†ëŠ”",
    "í•´ì„œ", "í•˜ê³ ", "í•´ìš”", "í•©ë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "ìˆì–´ìš”", "ì—†ì–´ìš”", "ê°™ì•„ìš”",
    "ì´ëŸ°", "ì €ëŸ°", "ê·¸ëŸ°", "ì–´ë–¤", "ë¬´ìŠ¨", "ì™œ", "ì–´ë””", "ì–¸ì œ", "ì–´ë–»ê²Œ",
    "ê·¼ë°", "ê·¸ë˜ì„œ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ê·¸ë˜ë„",
    "ìˆì–´", "ì—†ì–´", "í•˜ë©´", "ì´ìš©", "ì‚¬ìš©", "ì •ë„", "ì´ìƒ", "ê³„ì†", "ë‹¤ì‹œ", "ì²˜ìŒ", "ë§ˆì§€ë§‰",
    "ë„¤ì´ë²„", "ì›¹íˆ°", "ì¿ í‚¤", "ë§Œí™”", "ì‘í’ˆ", "ì¢‹ì•„", "ì½ê³ ", "ë³´ê³ ", "í•´ì„œ", "í•˜ê³ "
}

# ----------------------------
# í† í”½ í‚¤ì›Œë“œ ì •ì˜
# ----------------------------
TOPIC_KEYWORDS = {
    "ğŸ“š ì½˜í…ì¸ ": ["ì‘í’ˆ", "ì—°ì¬", "ì™„ê²°", "ìŠ¤í† ë¦¬", "ë‚´ìš©", "ì¬ë¯¸", "ê·¸ë¦¼", "í€„ë¦¬í‹°", "ì‹ ì‘", "ì¶”ì²œ", "ì‘ê°€", "íšŒì°¨", "ì¶œì‹œ", "ë³´ê³ ì‹¶", "ì½ê³ ì‹¶", "ê¸°ë‹¤", "ì‹œì¦Œ", "ì—í”¼ì†Œë“œ", "ìºë¦­í„°", "ê²°ë§", "ì „ì‘", "í›„ì†", "ì™¸ì „", "ì¬ë°Œ", "ì¬ë¯¸ìˆ", "ì›¹íˆ°"],
    "ğŸ’° ê²°ì œ/ê°€ê²©": ["ê²°ì œ", "ëˆ", "ìœ ë£Œ", "ë¬´ë£Œ", "ê°€ê²©", "ë¹„ì‹¸", "ë¹„ìš©", "ì½”ì¸", "ì¶©ì „", "í™˜ë¶ˆ", "êµ¬ë§¤", "êµ¬ë…", "ì´ìš©ê¶Œ", "í• ì¸", "ìºì‹œ", "ì¿ í‚¤", "ìœ ë£Œí™”", "ê³¼ê¸ˆ", "ìœ ë£Œê°€", "ë¬´ë£Œë¡œ", "ë¬´ë£Œë©´", "ìœ ë£Œë©´", "ëˆë‚´", "ëˆì„"],
    "ğŸ“º ê´‘ê³ ": ["ê´‘ê³ ", "ë°°ë„ˆ", "íŒì—…", "ìŠ¤í‚µ", "ê±´ë„ˆë›°ê¸°", "ë™ì˜ìƒê´‘ê³ ", "ì „ë©´ê´‘ê³ ", "ê´‘ê³ ê°€", "ê´‘ê³ ì—†", "ê´‘ê³ ì¢€", "ê´‘ê³ ë¥¼"],
    "ğŸ› ë²„ê·¸/ì˜¤ë¥˜": ["ë²„ê·¸", "ì˜¤ë¥˜", "ì—ëŸ¬", "ë ‰ê±¸", "íŠ•ê¹€", "íŠ•ê²¨", "ë©ˆì¶¤", "ì‘ë™ì•ˆ", "ëŠë ¤", "ë¡œë”©", "êº¼ì§", "ê°•ì œì¢…ë£Œ", "crash", "íŒ…ê¹€", "ë¬´í•œë¡œë”©", "ì•±êº¼", "ì‹¤í–‰ì•ˆ", "ë©ˆì¶°", "ë‹¤ìš´ë¨"],
    "ğŸ“± UI/UX": ["í™”ë©´", "ë²„íŠ¼", "ë””ìì¸", "ì¸í„°í˜ì´ìŠ¤", "ë©”ë‰´", "ë ˆì´ì•„ì›ƒ", "êµ¬ì„±", "ìœ„ì¹˜", "ì•„ì´ì½˜", "ìƒ‰ìƒ", "í°íŠ¸", "ê¸€ì”¨", "ìŠ¤í¬ë¡¤", "í„°ì¹˜", "ì¡°ì‘"],
    "ğŸ”” ì•Œë¦¼/í¸ì˜": ["ì•Œë¦¼", "í‘¸ì‹œ", "ë¶ë§ˆí¬", "ì €ì¥", "ê¸°ë¡", "ëª©ë¡", "ê²€ìƒ‰", "ì •ë ¬", "í•„í„°", "ê³µìœ ", "ë‹¤ìš´ë¡œë“œ", "ì˜¤í”„ë¼ì¸"],
}

# ----------------------------
# ê°ì„± í‚¤ì›Œë“œ ì •ì˜
# ----------------------------
POSITIVE_WORDS = {"ì¢‹ì•„", "ìµœê³ ", "ì¬ë°Œ", "ì¬ë¯¸ìˆ", "í¸ë¦¬", "í¸í•´", "ë§Œì¡±", "ì¶”ì²œ", "êµ¿", "ëŒ€ë°•", "ì‚¬ë‘", "ì™„ë²½", "í›Œë¥­", "ê°ì‚¬", "í–‰ë³µ", "ì¦ê±°"}
NEGATIVE_WORDS = {"ë³„ë¡œ", "ì‹«ì–´", "ìµœì•…", "ë¶ˆí¸", "ì§œì¦", "í™”ë‚˜", "ì‹¤ë§", "í›„íšŒ", "ì“°ë ˆê¸°", "í­ë§", "êµ¬ë¦¼", "ê°œì„ ", "ë‹µë‹µ", "ë¶ˆë§Œ", "ì§œì¦ë‚˜", "ì—ëŸ¬", "ë²„ê·¸"}

# ----------------------------
# ìš”ì²­ íŒ¨í„´ ì •ì˜
# ----------------------------
REQUEST_PATTERNS = [
    r"(.{2,20})(í•´ì£¼ì„¸ìš”|í•´ì¤˜ìš”|í•´ì£¼ê¸¸|ë°”ëë‹ˆë‹¤|ë°”ë˜ìš”|ì›í•©ë‹ˆë‹¤|ì›í•´ìš”|í–ˆìœ¼ë©´|ìœ¼ë©´ ì¢‹ê² |ë©´ ì¢‹ê² |í•´ë‹¬ë¼|í•´ì¤¬ìœ¼ë©´)",
    r"(ì œë°œ|ë¶€íƒ).{0,20}(í•´ì£¼|ë°”ë|ì›)",
    r"(.{2,15})(ê¸°ëŠ¥|ì˜µì…˜).{0,5}(ì¶”ê°€|ë„£ì–´|ë§Œë“¤ì–´)",
]

# ----------------------------
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ----------------------------
def simple_tokenizer(text):
    tokens = re.findall(r"[ê°€-í£]{2,}", str(text))
    tokens = [t for t in tokens if t not in STOPWORDS and len(t) >= 2]
    return tokens

def extract_bigrams(text):
    """í‚¤ì›Œë“œ ì¡°í•© (ë°”ì´ê·¸ë¨) ì¶”ì¶œ"""
    tokens = simple_tokenizer(text)
    bigrams = []
    for i in range(len(tokens) - 1):
        bigram = f"{tokens[i]} + {tokens[i+1]}"
        bigrams.append(bigram)
    return bigrams

def extract_trigrams(text):
    """í‚¤ì›Œë“œ ì¡°í•© (íŠ¸ë¦¬ê·¸ë¨ - 3ë‹¨ì–´) ì¶”ì¶œ"""
    tokens = simple_tokenizer(text)
    trigrams = []
    for i in range(len(tokens) - 2):
        trigram = f"{tokens[i]} + {tokens[i+1]} + {tokens[i+2]}"
        trigrams.append(trigram)
    return trigrams

@st.cache_data(ttl=86400, show_spinner=False)
def load_default_data():
    try:
        csv_path = os.path.join(os.path.dirname(__file__), "default_reviews.csv")
        df = pd.read_csv(csv_path)
        df["at"] = pd.to_datetime(df["at"])
        return df
    except Exception as e:
        st.error(f"ê¸°ë³¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=7200, show_spinner=False)
def get_reviews_cached(app_id, count=1000):
    result = []
    continuation_token = None
    
    try:
        while len(result) < count:
            batch_size = min(100, count - len(result))
            review_batch, continuation_token = reviews(
                app_id, lang="ko", country="kr",
                sort=Sort.NEWEST, count=batch_size,
                continuation_token=continuation_token
            )
            result.extend(review_batch)
            if not continuation_token:
                break
    except Exception as e:
        st.error(f"ë¦¬ë·° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()
    
    df = pd.DataFrame(result)
    if not df.empty:
        df["at"] = pd.to_datetime(df["at"])
        df["content"] = df["content"].astype(str)
    return df

# ----------------------------
# ë¶„ì„ í•¨ìˆ˜ë“¤
# ----------------------------
@st.cache_data(ttl=7200)
def analyze_sentiment(df):
    """ê°ì„± ë¶„ì„"""
    results = []
    
    for _, row in df.iterrows():
        text = str(row["content"])
        score = row["score"]
        
        pos_count = sum(1 for w in POSITIVE_WORDS if w in text)
        neg_count = sum(1 for w in NEGATIVE_WORDS if w in text)
        
        if score >= 4:
            sentiment = "ê¸ì •"
        elif score <= 2:
            sentiment = "ë¶€ì •"
        else:
            if pos_count > neg_count:
                sentiment = "ê¸ì •"
            elif neg_count > pos_count:
                sentiment = "ë¶€ì •"
            else:
                sentiment = "ì¤‘ë¦½"
        
        results.append(sentiment)
    
    df = df.copy()
    df["sentiment"] = results
    return df

@st.cache_data(ttl=7200)
def analyze_topics(contents_tuple):
    """í† í”½ ë¶„ë¥˜ - ë¦¬ë·°ë³„ë¡œ ë¶„ë¥˜ (ë³µìˆ˜ í† í”½ í—ˆìš©)"""
    topic_priority = ["ğŸ“š ì½˜í…ì¸ ", "ğŸ’° ê²°ì œ/ê°€ê²©", "ğŸ“º ê´‘ê³ ", "ğŸ› ë²„ê·¸/ì˜¤ë¥˜", "ğŸ“± UI/UX", "ğŸ”” ì•Œë¦¼/í¸ì˜"]
    topic_data = {topic: [] for topic in topic_priority}
    
    # ìš”ì²­ íŒ¨í„´ (ì´ê²Œ ìˆìœ¼ë©´ ë²„ê·¸ê°€ ì•„ë‹˜)
    request_patterns = ["í•´ì£¼", "í•´ì¤˜", "ì‹¶ì–´", "ë°”ëŒ", "ì›í•´", "ìœ¼ë©´ ì¢‹", "ë©´ ì¢‹ê² ", "ì œë°œ", "ë¶€íƒ", "ì—†ìœ¼ë©´", "ìˆìœ¼ë©´"]
    
    for text in contents_tuple:
        text = str(text)
        is_request = any(p in text for p in request_patterns)
        
        # ë³µìˆ˜ í† í”½ í—ˆìš©
        for topic in topic_priority:
            keywords = TOPIC_KEYWORDS[topic]
            
            # ë²„ê·¸/ì˜¤ë¥˜ í† í”½ì€ ìš”ì²­ íŒ¨í„´ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ
            if topic == "ğŸ› ë²„ê·¸/ì˜¤ë¥˜" and is_request:
                continue
            
            if any(kw in text for kw in keywords):
                topic_data[topic].append(text)
    
    return topic_data

@st.cache_data(ttl=7200)
def extract_requests(contents_tuple):
    """ìš”ì²­ì‚¬í•­ ì¶”ì¶œ"""
    requests = []
    
    for text in contents_tuple:
        text = str(text)
        for pattern in REQUEST_PATTERNS:
            matches = re.findall(pattern, text)
            for match in matches:
                if isinstance(match, tuple):
                    request_text = "".join(match)
                else:
                    request_text = match
                if len(request_text) > 5:
                    requests.append(request_text)
    
    return Counter(requests).most_common(30)

@st.cache_data(ttl=7200)
def analyze_complaints_trigram(df):
    """ë¶ˆë§Œ í‚¤ì›Œë“œ ì¡°í•© ë¶„ì„ (1-2ì  ë¦¬ë·°, íŠ¸ë¦¬ê·¸ë¨ - 3ë‹¨ì–´ ì¡°í•©)"""
    negative_df = df[df["score"] <= 2]
    
    if negative_df.empty:
        return [], [], pd.DataFrame()
    
    bigrams = []
    trigrams = []
    for text in negative_df["content"]:
        bigrams += extract_bigrams(text)
        trigrams += extract_trigrams(text)
    
    return Counter(bigrams).most_common(30), Counter(trigrams).most_common(30), negative_df

@st.cache_data(ttl=7200)
def analyze_positive_bigram(df):
    """ê¸ì • í‚¤ì›Œë“œ ì¡°í•© ë¶„ì„ (4-5ì  ë¦¬ë·°, ë°”ì´ê·¸ë¨)"""
    positive_df = df[df["score"] >= 4]
    
    if positive_df.empty:
        return [], pd.DataFrame()
    
    bigrams = []
    for text in positive_df["content"]:
        bigrams += extract_bigrams(text)
    
    return Counter(bigrams).most_common(30), positive_df

@st.cache_data(ttl=7200)
def generate_wordcloud_image(word_freq_tuple, font_path):
    word_freq = dict(word_freq_tuple)
    try:
        wc = WordCloud(
            font_path=font_path,
            width=800, height=400,
            background_color="white",
            colormap="viridis",
            max_words=50
        )
        wc.generate_from_frequencies(word_freq)
        img_buffer = BytesIO()
        wc.to_image().save(img_buffer, format='PNG')
        img_buffer.seek(0)
        return img_buffer.getvalue()
    except:
        return None

@st.cache_data(ttl=7200)
def extract_keywords_cached(contents_tuple):
    tokens = []
    for text in contents_tuple:
        tokens += simple_tokenizer(text)
    return tokens

@st.cache_data(ttl=7200)
def calculate_co_occurrence(contents_tuple):
    co_occurrence = {}
    for text in contents_tuple:
        tokens = simple_tokenizer(text)
        for i in range(len(tokens) - 1):
            a, b = tokens[i], tokens[i + 1]
            if a != b:
                co_occurrence.setdefault(a, []).append(b)
    return co_occurrence

# ----------------------------
# ë©”ì¸ ë¶„ì„ í‘œì‹œ í•¨ìˆ˜
# ----------------------------
def display_analysis(df, app_name="", data_info=""):
    if df.empty:
        st.error("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if data_info:
        st.info(data_info)
    
    st.success(f"âœ… **{len(df):,}ê±´** ë¦¬ë·° ë¶„ì„ ì™„ë£Œ! {f'({app_name})' if app_name else ''}")
    
    # ê°ì„± ë¶„ì„ ì ìš©
    df = analyze_sentiment(df)
    contents_tuple = tuple(df["content"].tolist())
    
    # íƒ­ êµ¬ì„±
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        "ğŸ“ˆ í†µê³„", "ğŸ˜Š ê°ì„±ë¶„ì„", "ğŸ“‚ í† í”½ë¶„ë¥˜", "ğŸ˜¤ ë¶ˆë§Œë¶„ì„", "ğŸ™ ìš”ì²­ì‚¬í•­", "ğŸ’¬ í‚¤ì›Œë“œ", "ğŸ“ ë¦¬ë·°"
    ])
    
    # ----------------------------
    # íƒ­ 1: í†µê³„
    # ----------------------------
    with tab1:
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ì´ ë¦¬ë·°", f"{len(df):,}")
        with col2:
            st.metric("í‰ê·  í‰ì ", f"{df['score'].mean():.1f}â­")
        with col3:
            pos_ratio = (df["sentiment"] == "ê¸ì •").sum() / len(df) * 100
            st.metric("ê¸ì • ë¹„ìœ¨", f"{pos_ratio:.0f}%")
        with col4:
            neg_ratio = (df["sentiment"] == "ë¶€ì •").sum() / len(df) * 100
            st.metric("ë¶€ì • ë¹„ìœ¨", f"{neg_ratio:.0f}%")
        
        st.markdown("---")
        
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ—“ï¸ ë‚ ì§œë³„ ë¦¬ë·°")
            daily = df.groupby(df["at"].dt.date).size()
            st.line_chart(daily)
        
        with col2:
            st.subheader("â­ í‰ì  ë¶„í¬")
            scores = df["score"].value_counts().sort_index()
            st.bar_chart(scores)
    
    # ----------------------------
    # íƒ­ 2: ê°ì„± ë¶„ì„
    # ----------------------------
    with tab2:
        st.subheader("ğŸ˜Š ê°ì„± ë¶„ì„ ê²°ê³¼")
        
        col1, col2 = st.columns(2)
        
        with col1:
            sentiment_counts = df["sentiment"].value_counts()
            st.markdown("#### ê°ì„± ë¶„í¬")
            
            for sentiment, count in sentiment_counts.items():
                pct = count / len(df) * 100
                if sentiment == "ê¸ì •":
                    st.success(f"ğŸ˜Š ê¸ì •: **{count:,}ê±´** ({pct:.1f}%)")
                elif sentiment == "ë¶€ì •":
                    st.error(f"ğŸ˜¤ ë¶€ì •: **{count:,}ê±´** ({pct:.1f}%)")
                else:
                    st.warning(f"ğŸ˜ ì¤‘ë¦½: **{count:,}ê±´** ({pct:.1f}%)")
        
        with col2:
            st.markdown("#### í‰ì ë³„ ê°ì„±")
            sentiment_by_score = df.groupby(["score", "sentiment"]).size().unstack(fill_value=0)
            st.dataframe(sentiment_by_score, use_container_width=True)
        
        st.markdown("---")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ğŸ˜Š ê¸ì • ë¦¬ë·° í‚¤ì›Œë“œ ì¡°í•©")
            pos_bigrams, _ = analyze_positive_bigram(df)
            if pos_bigrams:
                pos_df = pd.DataFrame(pos_bigrams[:15], columns=["í‚¤ì›Œë“œ ì¡°í•©", "ë¹ˆë„"])
                st.dataframe(pos_df, use_container_width=True, hide_index=True)
        
        with col2:
            st.markdown("#### ğŸ˜¤ ë¶€ì • ë¦¬ë·° í‚¤ì›Œë“œ ì¡°í•©")
            neg_bigrams, neg_trigrams, _ = analyze_complaints_trigram(df)
            if neg_bigrams:
                neg_df = pd.DataFrame(neg_bigrams[:15], columns=["í‚¤ì›Œë“œ ì¡°í•©", "ë¹ˆë„"])
                st.dataframe(neg_df, use_container_width=True, hide_index=True)
    
    # ----------------------------
    # íƒ­ 3: í† í”½ ë¶„ë¥˜ (ì„¸ë¡œ ë‚˜ì—´)
    # ----------------------------
    with tab3:
        st.subheader("ğŸ“‚ í† í”½ë³„ ë¦¬ë·° ë¶„ë¥˜")
        st.caption("ë¦¬ë·°ê°€ ì–´ë–¤ ì£¼ì œì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ëŠ”ì§€ ë¶„ë¥˜í•©ë‹ˆë‹¤.")
        
        topic_data = analyze_topics(contents_tuple)
        
        # í† í”½ë³„ ê°œìˆ˜ ì •ë ¬
        sorted_topics = sorted(topic_data.items(), key=lambda x: len(x[1]), reverse=True)
        
        # ì „ì²´ ìš”ì•½
        st.markdown("#### ğŸ“Š í† í”½ë³„ ì–¸ê¸‰ëŸ‰ ìš”ì•½")
        summary_data = []
        for topic, reviews_list in sorted_topics:
            summary_data.append({"í† í”½": topic, "ê±´ìˆ˜": len(reviews_list), "ë¹„ìœ¨": f"{len(reviews_list)/len(df)*100:.1f}%"})
        st.dataframe(pd.DataFrame(summary_data), use_container_width=True, hide_index=True)
        
        st.markdown("---")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„¸ë¡œ ë‚˜ì—´
        for topic, reviews_list in sorted_topics:
            with st.expander(f"{topic} ({len(reviews_list):,}ê±´)", expanded=False):
                if reviews_list:
                    # í•´ë‹¹ í† í”½ í‚¤ì›Œë“œ í‘œì‹œ
                    keywords = TOPIC_KEYWORDS[topic]
                    st.caption(f"ğŸ”‘ ê´€ë ¨ í‚¤ì›Œë“œ: {', '.join(keywords[:10])}")
                    
                    st.markdown("**ğŸ“‹ ëŒ€í‘œ ë¦¬ë·°:**")
                    for i, review in enumerate(reviews_list[:10], 1):
                        truncated = review[:150] + "..." if len(review) > 150 else review
                        st.text(f"{i}. {truncated}")
                else:
                    st.info("í•´ë‹¹ í† í”½ì˜ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ----------------------------
    # íƒ­ 4: ë¶ˆë§Œ ë¶„ì„ (í‚¤ì›Œë“œ ì¡°í•©)
    # ----------------------------
    with tab4:
        st.subheader("ğŸ˜¤ ë¶ˆë§Œ ì‚¬í•­ ì§‘ì¤‘ ë¶„ì„")
        st.caption("1~2ì  ë¦¬ë·°ì—ì„œ í‚¤ì›Œë“œ ì¡°í•©ì„ ë¶„ì„í•˜ì—¬ êµ¬ì²´ì ì¸ ë¶ˆë§Œ í¬ì¸íŠ¸ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.")
        
        neg_bigrams, neg_trigrams, neg_df = analyze_complaints_trigram(df)
        
        st.markdown(f"#### ğŸ”´ ë¶ˆë§Œ ë¦¬ë·° ìˆ˜: **{len(neg_df):,}ê±´** ({len(neg_df)/len(df)*100:.1f}%)")
        
        st.markdown("---")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ğŸ”¥ ë¶ˆë§Œ í‚¤ì›Œë“œ ì¡°í•© (2ë‹¨ì–´)")
            st.caption("ì–´ë–¤ ë‹¨ì–´ë“¤ì´ í•¨ê»˜ ì–¸ê¸‰ë˜ëŠ”ì§€ íŒŒì•…í•©ë‹ˆë‹¤.")
            if neg_bigrams:
                neg_bigram_df = pd.DataFrame(neg_bigrams[:20], columns=["í‚¤ì›Œë“œ ì¡°í•©", "ë¹ˆë„"])
                st.dataframe(neg_bigram_df, use_container_width=True, hide_index=True)
        
        with col2:
            st.markdown("#### ğŸ”¥ ë¶ˆë§Œ ë§¥ë½ íŒŒì•… (3ë‹¨ì–´)")
            st.caption("ë” êµ¬ì²´ì ì¸ ë§¥ë½ì„ íŒŒì•…í•©ë‹ˆë‹¤. ì˜ˆ: 'ì•± + ê»ë‹¤ + ì¼œë„'")
            if neg_trigrams:
                neg_trigram_df = pd.DataFrame(neg_trigrams[:20], columns=["í‚¤ì›Œë“œ ì¡°í•©", "ë¹ˆë„"])
                st.dataframe(neg_trigram_df, use_container_width=True, hide_index=True)
        
        st.markdown("---")
        st.markdown("#### ğŸ’¡ ì£¼ìš” ë¶ˆë§Œ íŒ¨í„´ í•´ì„")
        
        col1, col2 = st.columns(2)
        with col1:
            if neg_bigrams:
                st.markdown("**2ë‹¨ì–´ ì¡°í•© TOP 5:**")
                for i, (bigram, count) in enumerate(neg_bigrams[:5], 1):
                    st.markdown(f"{i}. **{bigram}** ({count}íšŒ)")
        
        with col2:
            if neg_trigrams:
                st.markdown("**3ë‹¨ì–´ ì¡°í•© TOP 5:**")
                for i, (trigram, count) in enumerate(neg_trigrams[:5], 1):
                    st.markdown(f"{i}. **{trigram}** ({count}íšŒ)")
        
        st.markdown("---")
        st.markdown(f"#### ğŸ“‹ ë¶ˆë§Œ ë¦¬ë·° ì›ë¬¸ (ì „ì²´ {len(neg_df):,}ê±´)")
        
        if not neg_df.empty:
            # ê²€ìƒ‰ í•„í„°
            search_complaint = st.text_input("ğŸ” ë¶ˆë§Œ ë¦¬ë·° ë‚´ ê²€ìƒ‰", key="complaint_search")
            
            filtered_neg = neg_df.copy()
            if search_complaint:
                filtered_neg = filtered_neg[filtered_neg["content"].str.contains(search_complaint, na=False)]
            
            st.write(f"**{len(filtered_neg):,}ê±´** í‘œì‹œ")
            
            display_neg = filtered_neg[["at", "score", "content"]].copy()
            display_neg["at"] = display_neg["at"].dt.strftime("%Y-%m-%d")
            display_neg.columns = ["ë‚ ì§œ", "í‰ì ", "ë‚´ìš©"]
            st.dataframe(display_neg, use_container_width=True, hide_index=True, height=400)
    
    # ----------------------------
    # íƒ­ 5: ìš”ì²­ì‚¬í•­
    # ----------------------------
    with tab5:
        st.subheader("ğŸ™ ì‚¬ìš©ì ìš”ì²­ì‚¬í•­ ì¶”ì¶œ")
        st.caption("'~í•´ì£¼ì„¸ìš”', '~í–ˆìœ¼ë©´ ì¢‹ê² ì–´ìš”' ë“±ì˜ íŒ¨í„´ì—ì„œ ì‚¬ìš©ì ë‹ˆì¦ˆë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.")
        
        requests = extract_requests(contents_tuple)
        
        if requests:
            st.markdown(f"#### ì´ **{len(requests)}ê°œ** ìš”ì²­ì‚¬í•­ ë°œê²¬")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### ğŸ“Œ ì£¼ìš” ìš”ì²­ì‚¬í•­")
                req_df = pd.DataFrame(requests[:15], columns=["ìš”ì²­ ë‚´ìš©", "ì–¸ê¸‰ íšŸìˆ˜"])
                st.dataframe(req_df, use_container_width=True, hide_index=True)
            
            with col2:
                st.markdown("#### ğŸ“Š ìš”ì²­ ë¹ˆë„")
                req_chart = pd.DataFrame(requests[:10], columns=["ìš”ì²­", "íšŸìˆ˜"]).set_index("ìš”ì²­")
                st.bar_chart(req_chart)
            
            st.markdown("---")
            st.markdown("#### ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸")
            
            if requests:
                top_requests = [r[0] for r in requests[:5]]
                st.markdown("**ì‚¬ìš©ìë“¤ì´ ê°€ì¥ ì›í•˜ëŠ” ê²ƒ:**")
                for i, req in enumerate(top_requests, 1):
                    st.markdown(f"{i}. {req}")
        else:
            st.info("ì¶”ì¶œëœ ìš”ì²­ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ----------------------------
    # íƒ­ 6: í‚¤ì›Œë“œ
    # ----------------------------
    with tab6:
        st.subheader("ğŸ’¬ ì „ì²´ í‚¤ì›Œë“œ ë¶„ì„")
        
        tokens = extract_keywords_cached(contents_tuple)
        counter = Counter(tokens)
        common_words = counter.most_common(30)
        
        if common_words:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                img_bytes = generate_wordcloud_image(tuple(common_words), FONT_PATH)
                if img_bytes:
                    st.image(img_bytes, use_container_width=True)
            
            with col2:
                keyword_df = pd.DataFrame(common_words, columns=["í‚¤ì›Œë“œ", "ë¹ˆë„"])
                st.dataframe(keyword_df, use_container_width=True, hide_index=True)
        
        st.markdown("---")
        st.markdown("#### ğŸ”— ì—°ê´€ í‚¤ì›Œë“œ")
        
        co_occurrence = calculate_co_occurrence(contents_tuple)
        related_words = []
        for k, v in counter.most_common(15):
            related = Counter(co_occurrence.get(k, [])).most_common(5)
            related_words.append({
                "í‚¤ì›Œë“œ": k,
                "ë¹ˆë„": v,
                "ì—°ê´€ë‹¨ì–´": ", ".join([f"{r[0]}({r[1]})" for r in related]) if related else "-"
            })
        
        st.dataframe(pd.DataFrame(related_words), use_container_width=True, hide_index=True)
    
    # ----------------------------
    # íƒ­ 7: ë¦¬ë·° ì›ë¬¸
    # ----------------------------
    with tab7:
        st.subheader("ğŸ“ ë¦¬ë·° ì›ë¬¸")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            score_filter = st.multiselect("í‰ì ", [1,2,3,4,5], default=[1,2,3,4,5])
        with col2:
            sentiment_filter = st.multiselect("ê°ì„±", ["ê¸ì •", "ì¤‘ë¦½", "ë¶€ì •"], default=["ê¸ì •", "ì¤‘ë¦½", "ë¶€ì •"])
        with col3:
            keyword = st.text_input("ê²€ìƒ‰")
        
        filtered = df[df["score"].isin(score_filter) & df["sentiment"].isin(sentiment_filter)]
        if keyword:
            filtered = filtered[filtered["content"].str.contains(keyword, na=False)]
        
        st.write(f"**{len(filtered):,}ê±´**")
        
        display_df = filtered.head(100)[["at", "score", "sentiment", "content"]].copy()
        display_df["at"] = display_df["at"].dt.strftime("%Y-%m-%d")
        display_df.columns = ["ë‚ ì§œ", "í‰ì ", "ê°ì„±", "ë‚´ìš©"]
        st.dataframe(display_df, use_container_width=True, hide_index=True)

# ----------------------------
# ë©”ì¸ UI
# ----------------------------
st.title("ğŸ“Š ê²½ìŸì‚¬ ì•± ë¦¬ë·° ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
st.caption("Google Play Store ë¦¬ë·°ë¥¼ ë¶„ì„í•˜ì—¬ ê²½ìŸì‚¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
    
    st.markdown("---")
    
    # ì•± ID ì…ë ¥ (í•­ìƒ í™œì„±í™”)
    st.markdown("#### ğŸ” ì•± ID ì…ë ¥")
    app_id_input = st.text_input(
        "Google Play ì•± ID",
        placeholder="com.example.app",
        help="Google Play Store URLì—ì„œ id= ë’¤ì˜ ê°’"
    )
    
    # ìƒ˜í”Œ ì•± ID
    st.markdown("##### ğŸ“‹ ìƒ˜í”Œ ì•± ID")
    st.code("com.nhn.android.webtoon", language=None)
    st.caption("â†‘ ë„¤ì´ë²„ ì›¹íˆ°")
    st.code("com.kakaopage.app", language=None)
    st.caption("â†‘ ì¹´ì¹´ì˜¤í˜ì´ì§€")
    st.code("com.initialcoms.ridi", language=None)
    st.caption("â†‘ ë¦¬ë””ë¶ìŠ¤")
    st.code("com.lezhin.comics", language=None)
    st.caption("â†‘ ë ˆì§„ì½”ë¯¹ìŠ¤")
    
    st.markdown("---")
    
    # ìˆ˜ì§‘ ì˜µì…˜
    st.markdown("#### âš™ï¸ ìˆ˜ì§‘ ì˜µì…˜")
    review_count = st.select_slider(
        "ìˆ˜ì§‘í•  ë¦¬ë·° ìˆ˜",
        options=[100, 300, 500, 700, 1000],
        value=500
    )
    
    st.markdown("---")
    
    # ë°ì´í„° ìˆ˜ì§‘ ë²„íŠ¼
    collect_btn = st.button(
        "ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", 
        type="primary", 
        use_container_width=True,
        disabled=(not app_id_input)
    )
    
    if not app_id_input:
        st.caption("ğŸ’¡ ì•± IDë¥¼ ì…ë ¥í•˜ë©´ ìˆ˜ì§‘ ë²„íŠ¼ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

# ë©”ì¸ ì½˜í…ì¸ 
# ìˆ˜ì§‘ ë²„íŠ¼ í´ë¦­ ì‹œ ë°ì´í„° ìˆ˜ì§‘
if collect_btn and app_id_input:
    with st.spinner(f"ğŸ“¥ {app_id_input} ë¦¬ë·° ìˆ˜ì§‘ ì¤‘... ({review_count}ê±´)"):
        df = get_reviews_cached(app_id_input, count=review_count)
        df = df.sort_values(by="at", ascending=False)
        st.session_state["collected_df"] = df
        st.session_state["collected_app"] = app_id_input

# ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í‘œì‹œ
if st.session_state.get("collected_df") is not None and not st.session_state["collected_df"].empty:
    display_analysis(st.session_state["collected_df"], st.session_state.get("collected_app", ""))

# ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°ì´í„° í‘œì‹œ
else:
    with st.spinner("ğŸ“¥ ê¸°ë³¸ ë°ì´í„° ë¡œë”© ì¤‘..."):
        df = load_default_data()
    display_analysis(df, "ë„¤ì´ë²„ ì›¹íˆ°", "ğŸ“Œ **ê¸°ë³¸ ë°ì´í„°**: ë„¤ì´ë²„ ì›¹íˆ° ë¦¬ë·° 1,000ê±´ (2025.01.19 ê¸°ì¤€)")

st.markdown("---")
st.caption("Made with â¤ï¸ using Streamlit | ë°ì´í„°: Google Play Store")

import streamlit as st
from google_play_scraper import reviews, Sort
import pandas as pd
from collections import Counter
from wordcloud import WordCloud
import re
import os
from io import BytesIO

# ----------------------------
# í˜ì´ì§€ ì„¤ì •
# ----------------------------
st.set_page_config(
    page_title="ê²½ìŸì‚¬ ì•± ë¦¬ë·° ë¶„ì„",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ----------------------------
# ë°˜ì‘í˜• CSS ìŠ¤íƒ€ì¼
# ----------------------------
st.markdown("""
<style>
/* ì „ì²´ í°íŠ¸ í¬ê¸° ì¶•ì†Œ */
html, body, [class*="css"] {
    font-size: 14px;
}

/* ì œëª© í¬ê¸° ì¡°ì • */
h1 {
    font-size: 1.6rem !important;
}
h2 {
    font-size: 1.3rem !important;
}
h3, .stSubheader {
    font-size: 1.1rem !important;
}
h4 {
    font-size: 1rem !important;
}

/* ì‚¬ì´ë“œë°” ìµœì í™” */
[data-testid="stSidebar"] {
    min-width: 280px;
    max-width: 320px;
}
[data-testid="stSidebar"] .stMarkdown {
    font-size: 13px;
}
[data-testid="stSidebar"] code {
    font-size: 11px;
    padding: 4px 8px;
}
[data-testid="stSidebar"] .stCaption {
    font-size: 11px;
}

/* ë©”íŠ¸ë¦­ ì¹´ë“œ í¬ê¸° ì¡°ì • */
[data-testid="stMetricValue"] {
    font-size: 1.3rem !important;
}
[data-testid="stMetricLabel"] {
    font-size: 0.85rem !important;
}

/* íƒ­ ê³ ì • (ìŠ¤í¬ë¡¤ ì‹œ ìƒë‹¨ì— ê³ ì •) */
.stTabs [data-baseweb="tab-list"] {
    position: sticky;
    top: 0;
    background: white;
    z-index: 999;
    padding: 10px 0;
    border-bottom: 1px solid #eee;
}

/* íƒ­ í¬ê¸° ì¡°ì • */
.stTabs [data-baseweb="tab-list"] button {
    font-size: 13px;
    padding: 8px 12px;
}

/* í…Œì´ë¸” í°íŠ¸ í¬ê¸° */
.stDataFrame {
    font-size: 12px;
}

/* ë²„íŠ¼ í¬ê¸° */
.stButton > button {
    font-size: 13px;
    padding: 8px 16px;
}

/* ì…ë ¥ í•„ë“œ ê¸°ë³¸ ìŠ¤íƒ€ì¼ */
.stTextInput input {
    font-size: 13px;
}

/* í‚¤ì›Œë“œ ì…ë ¥ í•„ë“œ - ê¸°ë³¸ ë…¹ìƒ‰ í…Œë‘ë¦¬ */
.keyword-input input {
    border: 2px solid #28a745 !important;
    border-radius: 5px !important;
}

/* í‚¤ì›Œë“œ ì…ë ¥ í•„ë“œ - í¬ì»¤ìŠ¤ ë° ì…ë ¥ ì‹œ ë…¸ë€ìƒ‰ */
.keyword-input input:focus,
.keyword-input input:not(:placeholder-shown) {
    border-color: #ffc107 !important;
    box-shadow: 0 0 0 2px rgba(255, 193, 7, 0.3) !important;
}

/* ëª¨ë°”ì¼ ëŒ€ì‘ (ì•„ì´í° 15: 393px) */
@media (max-width: 768px) {
    html, body, [class*="css"] {
        font-size: 12px;
    }
    h1 {
        font-size: 1.4rem !important;
    }
    h2 {
        font-size: 1.2rem !important;
    }
    [data-testid="stSidebar"] {
        min-width: 100%;
    }
    [data-testid="stMetricValue"] {
        font-size: 1.1rem !important;
    }
    .stTabs [data-baseweb="tab-list"] button {
        font-size: 11px;
        padding: 6px 8px;
    }
}

/* ë§¥ë¶ ì—ì–´ (1440px ì´í•˜) */
@media (max-width: 1440px) {
    [data-testid="stSidebar"] {
        min-width: 260px;
        max-width: 280px;
    }
}

/* ê°„ê²© ìµœì í™” - Streamlit Cloud í—¤ë” ê³ ë ¤ */
.block-container {
    padding-top: 2.5rem;
    padding-bottom: 1rem;
}

/* ë©”ì¸ ì»¨í…ì¸  ì˜ì—­ ìƒë‹¨ ì—¬ë°± */
.main .block-container {
    padding-top: 3rem;
}

.stMarkdown {
    line-height: 1.5;
}

/* ë°ì´í„°í”„ë ˆì„ ì»´íŒ©íŠ¸ */
[data-testid="stDataFrame"] td, 
[data-testid="stDataFrame"] th {
    padding: 4px 8px !important;
    font-size: 12px !important;
}
</style>
""", unsafe_allow_html=True)

# ----------------------------
# í°íŠ¸ ê²½ë¡œ ì„¤ì •
# ----------------------------
def get_font_path():
    possible_paths = [
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf",
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
    ]
    for path in possible_paths:
        if os.path.exists(path):
            return path
    return None

FONT_PATH = get_font_path()

# ----------------------------
# ì•± ëª©ë¡ ì •ì˜
# ----------------------------
APP_LIST = {
    "ë„¤ì´ë²„ ì›¹íˆ°": "com.nhn.android.webtoon",
    "ì¹´ì¹´ì˜¤í˜ì´ì§€": "com.kakaopage.app",
    "ë ˆì§„ì½”ë¯¹ìŠ¤": "com.lezhin.comics",
    "ë¦¬ë””ë¶ìŠ¤": "com.initialcoms.ridi",
}

# ----------------------------
# ë¶ˆìš©ì–´ ì •ì˜
# ----------------------------
STOPWORDS = {
    "ë„ˆë¬´", "ì •ë§", "ì§„ì§œ", "ë§¤ìš°", "ì•„ì£¼", "ì™„ì „", "ë˜ê²Œ", "ê½¤", "ì¢€", "ì•½ê°„", "ì‚´ì§",
    "ê·¸ëƒ¥", "ì´ê±°", "ì €ê±°", "ê·¸ê²ƒ", "ì´ê²ƒ", "ì €ê²ƒ", "í•˜ëŠ”", "ìˆëŠ”", "ì—†ëŠ”",
    "í•´ì„œ", "í•˜ê³ ", "í•´ìš”", "í•©ë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "ìˆì–´ìš”", "ì—†ì–´ìš”", "ê°™ì•„ìš”",
    "ì´ëŸ°", "ì €ëŸ°", "ê·¸ëŸ°", "ì–´ë–¤", "ë¬´ìŠ¨", "ì™œ", "ì–´ë””", "ì–¸ì œ", "ì–´ë–»ê²Œ",
    "ê·¼ë°", "ê·¸ë˜ì„œ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ê·¸ë˜ë„",
    "ìˆì–´", "ì—†ì–´", "í•˜ë©´", "ì´ìš©", "ì‚¬ìš©", "ì •ë„", "ì´ìƒ", "ê³„ì†", "ë‹¤ì‹œ", "ì²˜ìŒ", "ë§ˆì§€ë§‰",
    "ë„¤ì´ë²„", "ì›¹íˆ°", "ì¿ í‚¤", "ë§Œí™”", "ì‘í’ˆ", "ì¢‹ì•„", "ì½ê³ ", "ë³´ê³ ", "í•´ì„œ", "í•˜ê³ "
}

# ----------------------------
# í† í”½ í‚¤ì›Œë“œ ì •ì˜
# ----------------------------
TOPIC_KEYWORDS = {
    "ğŸ“š ì½˜í…ì¸ ": ["ì‘í’ˆ", "ì—°ì¬", "ì™„ê²°", "ìŠ¤í† ë¦¬", "ë‚´ìš©", "ì¬ë¯¸", "ê·¸ë¦¼", "í€„ë¦¬í‹°", "ì‹ ì‘", "ì¶”ì²œ", "ì‘ê°€", "íšŒì°¨", "ì¶œì‹œ", "ë³´ê³ ì‹¶", "ì½ê³ ì‹¶", "ê¸°ë‹¤", "ì‹œì¦Œ", "ì—í”¼ì†Œë“œ", "ìºë¦­í„°", "ê²°ë§", "ì „ì‘", "í›„ì†", "ì™¸ì „", "ì¬ë°Œ", "ì¬ë¯¸ìˆ", "ì›¹íˆ°"],
    "ğŸ’° ê²°ì œ/ê°€ê²©": ["ê²°ì œ", "ëˆ", "ìœ ë£Œ", "ë¬´ë£Œ", "ê°€ê²©", "ë¹„ì‹¸", "ë¹„ìš©", "ì½”ì¸", "ì¶©ì „", "í™˜ë¶ˆ", "êµ¬ë§¤", "êµ¬ë…", "ì´ìš©ê¶Œ", "í• ì¸", "ìºì‹œ", "ì¿ í‚¤", "ìœ ë£Œí™”", "ê³¼ê¸ˆ", "ìœ ë£Œê°€", "ë¬´ë£Œë¡œ", "ë¬´ë£Œë©´", "ìœ ë£Œë©´", "ëˆë‚´", "ëˆì„"],
    "ğŸ“º ê´‘ê³ ": ["ê´‘ê³ ", "ë°°ë„ˆ", "íŒì—…", "ìŠ¤í‚µ", "ê±´ë„ˆë›°ê¸°", "ë™ì˜ìƒê´‘ê³ ", "ì „ë©´ê´‘ê³ ", "ê´‘ê³ ê°€", "ê´‘ê³ ì—†", "ê´‘ê³ ì¢€", "ê´‘ê³ ë¥¼"],
    "ğŸ› ë²„ê·¸/ì˜¤ë¥˜": ["ë²„ê·¸", "ì˜¤ë¥˜", "ì—ëŸ¬", "ë ‰ê±¸", "íŠ•ê¹€", "íŠ•ê²¨", "ë©ˆì¶¤", "ì‘ë™ì•ˆ", "ëŠë ¤", "ë¡œë”©", "êº¼ì§", "ê°•ì œì¢…ë£Œ", "crash", "íŒ…ê¹€", "ë¬´í•œë¡œë”©", "ì•±êº¼", "ì‹¤í–‰ì•ˆ", "ë©ˆì¶°", "ë‹¤ìš´ë¨"],
    "ğŸ“± UI/UX": ["í™”ë©´", "ë²„íŠ¼", "ë””ìì¸", "ì¸í„°í˜ì´ìŠ¤", "ë©”ë‰´", "ë ˆì´ì•„ì›ƒ", "êµ¬ì„±", "ìœ„ì¹˜", "ì•„ì´ì½˜", "ìƒ‰ìƒ", "í°íŠ¸", "ê¸€ì”¨", "ìŠ¤í¬ë¡¤", "í„°ì¹˜", "ì¡°ì‘"],
    "ğŸ”” ì•Œë¦¼/í¸ì˜": ["ì•Œë¦¼", "í‘¸ì‹œ", "ë¶ë§ˆí¬", "ì €ì¥", "ê¸°ë¡", "ëª©ë¡", "ê²€ìƒ‰", "ì •ë ¬", "í•„í„°", "ê³µìœ ", "ë‹¤ìš´ë¡œë“œ", "ì˜¤í”„ë¼ì¸"],
}

# ----------------------------
# ê°ì„± í‚¤ì›Œë“œ ì •ì˜ (ê¸°ë³¸)
# ----------------------------
POSITIVE_WORDS = {"ì¢‹ì•„", "ìµœê³ ", "ì¬ë°Œ", "ì¬ë¯¸ìˆ", "í¸ë¦¬", "í¸í•´", "ë§Œì¡±", "ì¶”ì²œ", "êµ¿", "ëŒ€ë°•", "ì‚¬ë‘", "ì™„ë²½", "í›Œë¥­", "ê°ì‚¬", "í–‰ë³µ", "ì¦ê±°"}
NEGATIVE_WORDS = {"ë³„ë¡œ", "ì‹«ì–´", "ìµœì•…", "ë¶ˆí¸", "ì§œì¦", "í™”ë‚˜", "ì‹¤ë§", "í›„íšŒ", "ì“°ë ˆê¸°", "í­ë§", "êµ¬ë¦¼", "ê°œì„ ", "ë‹µë‹µ", "ë¶ˆë§Œ", "ì§œì¦ë‚˜", "ì—ëŸ¬", "ë²„ê·¸"}

# ----------------------------
# ì›¹íˆ°/ë§Œí™” íŠ¹í™” ê°ì„± í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ í¬í•¨)
# ----------------------------
WEBTOON_SENTIMENT = {
    "positive": {
        # ê¸°ë³¸ (weight 1-3)
        "ì¢‹ë‹¤": 1, "ì¢‹ì•„ìš”": 1, "ë§Œì¡±": 1,
        "ì¬ë°Œë‹¤": 2, "ì¬ë¯¸ìˆë‹¤": 2, "ì¶”ì²œ": 2, "ê°ë™": 2, "ëª°ì…": 2, "ì—¬ìš´": 2,
        "ê°•ì¶”": 3, "ìµœê³ ": 3, "ì™„ë²½": 3,
        # ì›¹íˆ°íŠ¹í™”
        "ì‘í™”ì¢‹ë‹¤": 2, "ì‘í™”ì¢‹ìŒ": 2, "ì‘í™”ë¯¸ì³¤ë‹¤": 3, "ì‘í™”ë¯¸ì¹¨": 3,
        "ìŠ¤í† ë¦¬íƒ„íƒ„": 3, "ì „ê°œê¹”ë”": 2, "ì—°ì¶œì¢‹ë‹¤": 2, "ì—°ì¶œì¢‹ìŒ": 2,
        "ìºë¦­í„°ë§¤ë ¥": 2, "ê°œì—°ì„±ìˆë‹¤": 2, "ì„¸ê³„ê´€íƒ„íƒ„": 3,
        "ë–¡ë°¥íšŒìˆ˜": 3, "ë‹¤ìŒí™”ê¸°ëŒ€": 2, "ì •ì£¼í–‰": 2, "ì‹œê°„ìˆœì‚­": 3,
        # ê·¹ë‹¨
        "ê°“ì‘": 3, "ëª…ì‘": 3, "ë ˆì „ë“œ": 3, "ì¸ìƒì›¹íˆ°": 3, "ì†Œë¦„": 3,
        # ì¶”ê°€ ë³€í˜•
        "ì¬ë°Œ": 2, "ì¬ë¯¸ìˆ": 2, "ì¢‹ì•„": 1, "ê°•ë ¥ì¶”ì²œ": 3, "ê¿€ì¼": 3,
        "ì‘í™”": 1, "ìŠ¤í† ë¦¬": 1, "ëª°ì…ê°": 2, "ê°ë™ì ": 2,
    },
    "negative": {
        # ê¸°ë³¸
        "ë…¸ì¼": 3, "ë³„ë¡œ": 1, "ì‹¤ë§": 2, "ì•„ì‰½ë‹¤": 1, "ì•„ì‰¬ì›€": 1,
        "ì§€ë£¨": 2, "ë‹µë‹µ": 2, "ë¹„ì¶”": 2, "ìµœì•…": 3, "ì¬ë¯¸ì—†ë‹¤": 2, "ì¬ë¯¸ì—†": 2,
        # ì›¹íˆ°íŠ¹í™”
        "ì‘í™”ë¶•ê´´": 3, "ì‘ë¶•": 3, "ìŠ¤í† ë¦¬ì‚°ìœ¼ë¡œ": 3, "ì‚°ìœ¼ë¡œ": 2,
        "ê°œì—°ì„±ì—†ë‹¤": 3, "ê°œì—°ì„±ì—†ìŒ": 3, "ì „ê°œëŠë¦¼": 2, "ê¸‰ì „ê°œ": 2,
        "ìºë¶•": 3, "ì„¤ì •ë¶•ê´´": 3, "ì§ˆì§ˆëˆë‹¤": 2, "ì§ˆì§ˆë”": 2,
        "ë–¡ë°¥ë°©ì¹˜": 3, "ëª°ì…ê¹¨ì§": 2,
        # ê·¹ë‹¨
        "í•˜ì°¨": 3, "ì‹œê°„ë‚­ë¹„": 3, "ëˆì•„ê¹Œì›€": 3, "ë°œì•”": 3, "ê°œë§ì‘": 3,
        # ì¶”ê°€ ë³€í˜•
        "ë…¸ì¼ì„": 3, "ë³„ë¡œì„": 1, "ì§€ë£¨í•¨": 2, "ì§€ë£¨í•´": 2,
    }
}

# ----------------------------
# ìš”ì²­ íŒ¨í„´ ì •ì˜
# ----------------------------
REQUEST_PATTERNS = [
    r"(.{2,20})(í•´ì£¼ì„¸ìš”|í•´ì¤˜ìš”|í•´ì£¼ê¸¸|ë°”ëë‹ˆë‹¤|ë°”ë˜ìš”|ì›í•©ë‹ˆë‹¤|ì›í•´ìš”|í–ˆìœ¼ë©´|ìœ¼ë©´ ì¢‹ê² |ë©´ ì¢‹ê² |í•´ë‹¬ë¼|í•´ì¤¬ìœ¼ë©´)",
    r"(ì œë°œ|ë¶€íƒ).{0,20}(í•´ì£¼|ë°”ë|ì›)",
    r"(.{2,15})(ê¸°ëŠ¥|ì˜µì…˜).{0,5}(ì¶”ê°€|ë„£ì–´|ë§Œë“¤ì–´)",
]

# ----------------------------
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ----------------------------
def simple_tokenizer(text):
    tokens = re.findall(r"[ê°€-í£]{2,}", str(text))
    tokens = [t for t in tokens if t not in STOPWORDS and len(t) >= 2]
    return tokens

def extract_bigrams(text):
    """í‚¤ì›Œë“œ ì¡°í•© (ë°”ì´ê·¸ë¨) ì¶”ì¶œ"""
    tokens = simple_tokenizer(text)
    bigrams = []
    for i in range(len(tokens) - 1):
        bigram = f"{tokens[i]} + {tokens[i+1]}"
        bigrams.append(bigram)
    return bigrams

def extract_trigrams(text):
    """í‚¤ì›Œë“œ ì¡°í•© (íŠ¸ë¦¬ê·¸ë¨ - 3ë‹¨ì–´) ì¶”ì¶œ"""
    tokens = simple_tokenizer(text)
    trigrams = []
    for i in range(len(tokens) - 2):
        trigram = f"{tokens[i]} + {tokens[i+1]} + {tokens[i+2]}"
        trigrams.append(trigram)
    return trigrams

@st.cache_data(ttl=86400, show_spinner="ê¸°ë³¸ ë°ì´í„° ë¡œë”©...")
def load_default_data():
    """ê¸°ë³¸ ë°ì´í„° ë¡œë“œ (CSVì— sentiment í¬í•¨ ì‹œ ì¦‰ì‹œ ë°˜í™˜)"""
    try:
        csv_path = os.path.join(os.path.dirname(__file__), "default_reviews.csv")
        df = pd.read_csv(csv_path)
        df["at"] = pd.to_datetime(df["at"])
        
        # CSVì— ì´ë¯¸ sentimentê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
        if "sentiment" in df.columns:
            return df
        
        # ì—†ìœ¼ë©´ ê°ì„±ë¶„ì„ ìˆ˜í–‰ (ìµœì´ˆ 1íšŒ)
        results = []
        pos_scores = []
        neg_scores = []
        
        for _, row in df.iterrows():
            text = str(row["content"])
            score = row["score"]
            
            pos_weight = sum(weight for word, weight in WEBTOON_SENTIMENT["positive"].items() if word in text)
            neg_weight = sum(weight for word, weight in WEBTOON_SENTIMENT["negative"].items() if word in text)
            
            pos_scores.append(pos_weight)
            neg_scores.append(neg_weight)
            
            if score >= 4:
                sentiment = "ë¶€ì •" if neg_weight >= 6 and neg_weight > pos_weight else "ê¸ì •"
            elif score <= 2:
                sentiment = "ê¸ì •" if pos_weight >= 6 and pos_weight > neg_weight else "ë¶€ì •"
            else:
                diff = pos_weight - neg_weight
                sentiment = "ê¸ì •" if diff >= 2 else ("ë¶€ì •" if diff <= -2 else "ì¤‘ë¦½")
            
            results.append(sentiment)
        
        df["sentiment"] = results
        df["pos_score"] = pos_scores
        df["neg_score"] = neg_scores
        
        return df
    except Exception as e:
        st.error(f"ê¸°ë³¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=7200, show_spinner=False)
def get_reviews_cached(app_id, count=1000):
    result = []
    continuation_token = None
    
    try:
        while len(result) < count:
            batch_size = min(100, count - len(result))
            review_batch, continuation_token = reviews(
                app_id, lang="ko", country="kr",
                sort=Sort.NEWEST, count=batch_size,
                continuation_token=continuation_token
            )
            result.extend(review_batch)
            if not continuation_token:
                break
    except Exception as e:
        st.error(f"ë¦¬ë·° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()
    
    df = pd.DataFrame(result)
    if not df.empty:
        df["at"] = pd.to_datetime(df["at"])
        df["content"] = df["content"].astype(str)
    return df

# ----------------------------
# ë¶„ì„ í•¨ìˆ˜ë“¤ (ìºì‹± ì ìš©)
# ----------------------------
@st.cache_data(ttl=7200, show_spinner=False)
def analyze_sentiment_basic_cached(df_json):
    """ê¸°ë³¸ ê°ì„± ë¶„ì„ (ìºì‹±ìš©)"""
    df = pd.read_json(df_json)
    results = []
    
    for _, row in df.iterrows():
        text = str(row["content"])
        score = row["score"]
        
        pos_count = sum(1 for w in POSITIVE_WORDS if w in text)
        neg_count = sum(1 for w in NEGATIVE_WORDS if w in text)
        
        if score >= 4:
            sentiment = "ê¸ì •"
        elif score <= 2:
            sentiment = "ë¶€ì •"
        else:
            if pos_count > neg_count:
                sentiment = "ê¸ì •"
            elif neg_count > pos_count:
                sentiment = "ë¶€ì •"
            else:
                sentiment = "ì¤‘ë¦½"
        
        results.append(sentiment)
    
    df["sentiment"] = results
    df["pos_score"] = 0
    df["neg_score"] = 0
    return df.to_json()

@st.cache_data(ttl=7200, show_spinner=False)
def analyze_sentiment_webtoon_cached(df_json):
    """ì›¹íˆ°/ë§Œí™” íŠ¹í™” ê°ì„± ë¶„ì„ (ìºì‹±ìš©)"""
    df = pd.read_json(df_json)
    results = []
    pos_scores = []
    neg_scores = []
    
    for _, row in df.iterrows():
        text = str(row["content"])
        score = row["score"]
        
        # ê°€ì¤‘ì¹˜ í•©ì‚°
        pos_weight = sum(weight for word, weight in WEBTOON_SENTIMENT["positive"].items() if word in text)
        neg_weight = sum(weight for word, weight in WEBTOON_SENTIMENT["negative"].items() if word in text)
        
        pos_scores.append(pos_weight)
        neg_scores.append(neg_weight)
        
        # í‰ì  ê¸°ë°˜ ê¸°ë³¸ íŒë‹¨ + ê°€ì¤‘ì¹˜ ë³´ì •
        if score >= 4:
            if neg_weight >= 6:
                sentiment = "ë¶€ì •" if neg_weight > pos_weight else "ê¸ì •"
            else:
                sentiment = "ê¸ì •"
        elif score <= 2:
            if pos_weight >= 6:
                sentiment = "ê¸ì •" if pos_weight > neg_weight else "ë¶€ì •"
            else:
                sentiment = "ë¶€ì •"
        else:
            diff = pos_weight - neg_weight
            if diff >= 2:
                sentiment = "ê¸ì •"
            elif diff <= -2:
                sentiment = "ë¶€ì •"
            else:
                sentiment = "ì¤‘ë¦½"
        
        results.append(sentiment)
    
    df["sentiment"] = results
    df["pos_score"] = pos_scores
    df["neg_score"] = neg_scores
    return df.to_json()

def analyze_sentiment_basic(df):
    """ê¸°ë³¸ ê°ì„± ë¶„ì„ (ë˜í¼)"""
    result_json = analyze_sentiment_basic_cached(df.to_json())
    return pd.read_json(result_json)

def analyze_sentiment_webtoon(df):
    """ì›¹íˆ°/ë§Œí™” íŠ¹í™” ê°ì„± ë¶„ì„ (ë˜í¼)"""
    result_json = analyze_sentiment_webtoon_cached(df.to_json())
    return pd.read_json(result_json)

def get_matched_keywords(text, is_webtoon_mode=False):
    """í…ìŠ¤íŠ¸ì—ì„œ ë§¤ì¹­ëœ ê°ì„± í‚¤ì›Œë“œ ì¶”ì¶œ"""
    if is_webtoon_mode:
        pos_matched = [(w, weight) for w, weight in WEBTOON_SENTIMENT["positive"].items() if w in text]
        neg_matched = [(w, weight) for w, weight in WEBTOON_SENTIMENT["negative"].items() if w in text]
    else:
        pos_matched = [(w, 1) for w in POSITIVE_WORDS if w in text]
        neg_matched = [(w, 1) for w in NEGATIVE_WORDS if w in text]
    return pos_matched, neg_matched

@st.cache_data(ttl=7200)
def analyze_topics(contents_tuple):
    """í† í”½ ë¶„ë¥˜ - ë¦¬ë·°ë³„ë¡œ ë¶„ë¥˜ (ë³µìˆ˜ í† í”½ í—ˆìš©)"""
    topic_priority = ["ğŸ“š ì½˜í…ì¸ ", "ğŸ’° ê²°ì œ/ê°€ê²©", "ğŸ“º ê´‘ê³ ", "ğŸ› ë²„ê·¸/ì˜¤ë¥˜", "ğŸ“± UI/UX", "ğŸ”” ì•Œë¦¼/í¸ì˜"]
    topic_data = {topic: [] for topic in topic_priority}
    
    # ìš”ì²­ íŒ¨í„´ (ì´ê²Œ ìˆìœ¼ë©´ ë²„ê·¸ê°€ ì•„ë‹˜)
    request_patterns = ["í•´ì£¼", "í•´ì¤˜", "ì‹¶ì–´", "ë°”ëŒ", "ì›í•´", "ìœ¼ë©´ ì¢‹", "ë©´ ì¢‹ê² ", "ì œë°œ", "ë¶€íƒ", "ì—†ìœ¼ë©´", "ìˆìœ¼ë©´"]
    
    for text in contents_tuple:
        text = str(text)
        is_request = any(p in text for p in request_patterns)
        
        # ë³µìˆ˜ í† í”½ í—ˆìš©
        for topic in topic_priority:
            keywords = TOPIC_KEYWORDS[topic]
            
            # ë²„ê·¸/ì˜¤ë¥˜ í† í”½ì€ ìš”ì²­ íŒ¨í„´ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ
            if topic == "ğŸ› ë²„ê·¸/ì˜¤ë¥˜" and is_request:
                continue
            
            if any(kw in text for kw in keywords):
                topic_data[topic].append(text)
    
    return topic_data

@st.cache_data(ttl=7200)
def extract_requests(contents_tuple):
    """ìš”ì²­ì‚¬í•­ ì¶”ì¶œ"""
    requests = []
    
    for text in contents_tuple:
        text = str(text)
        for pattern in REQUEST_PATTERNS:
            matches = re.findall(pattern, text)
            for match in matches:
                if isinstance(match, tuple):
                    request_text = "".join(match)
                else:
                    request_text = match
                if len(request_text) > 5:
                    requests.append(request_text)
    
    return Counter(requests).most_common(30)

@st.cache_data(ttl=7200)
def analyze_complaints_trigram(df):
    """ë¶ˆë§Œ í‚¤ì›Œë“œ ì¡°í•© ë¶„ì„ (1-2ì  ë¦¬ë·°, íŠ¸ë¦¬ê·¸ë¨ - 3ë‹¨ì–´ ì¡°í•©)"""
    negative_df = df[df["score"] <= 2]
    
    if negative_df.empty:
        return [], [], pd.DataFrame()
    
    bigrams = []
    trigrams = []
    for text in negative_df["content"]:
        bigrams += extract_bigrams(text)
        trigrams += extract_trigrams(text)
    
    return Counter(bigrams).most_common(30), Counter(trigrams).most_common(30), negative_df

@st.cache_data(ttl=7200)
def analyze_positive_bigram(df):
    """ê¸ì • í‚¤ì›Œë“œ ì¡°í•© ë¶„ì„ (4-5ì  ë¦¬ë·°, ë°”ì´ê·¸ë¨)"""
    positive_df = df[df["score"] >= 4]
    
    if positive_df.empty:
        return [], pd.DataFrame()
    
    bigrams = []
    for text in positive_df["content"]:
        bigrams += extract_bigrams(text)
    
    return Counter(bigrams).most_common(30), positive_df

@st.cache_data(ttl=7200)
def generate_wordcloud_image(word_freq_tuple, font_path):
    word_freq = dict(word_freq_tuple)
    try:
        wc = WordCloud(
            font_path=font_path,
            width=800, height=400,
            background_color="white",
            colormap="viridis",
            max_words=50
        )
        wc.generate_from_frequencies(word_freq)
        img_buffer = BytesIO()
        wc.to_image().save(img_buffer, format='PNG')
        img_buffer.seek(0)
        return img_buffer.getvalue()
    except:
        return None

@st.cache_data(ttl=7200)
def extract_keywords_cached(contents_tuple):
    tokens = []
    for text in contents_tuple:
        tokens += simple_tokenizer(text)
    return tokens

@st.cache_data(ttl=7200)
def calculate_co_occurrence(contents_tuple):
    co_occurrence = {}
    for text in contents_tuple:
        tokens = simple_tokenizer(text)
        for i in range(len(tokens) - 1):
            a, b = tokens[i], tokens[i + 1]
            if a != b:
                co_occurrence.setdefault(a, []).append(b)
    return co_occurrence

# ----------------------------
# ë©”ì¸ ë¶„ì„ í‘œì‹œ í•¨ìˆ˜
# ----------------------------
def display_analysis(df, app_name="", data_info=""):
    if df.empty:
        st.error("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if data_info:
        st.info(data_info)
    
    # ì›¹íˆ° íŠ¹í™” í‚¤ì›Œë“œ help í…ìŠ¤íŠ¸ (í‘œ í˜•íƒœ)
    webtoon_help = """
ã€ê¸ì • í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜)ã€‘
â€¢ ê¸°ë³¸: ì¢‹ë‹¤(1), ì¬ë°Œë‹¤(2), ê°•ì¶”(3), ìµœê³ (3)
â€¢ ì›¹íˆ°íŠ¹í™”: ì‘í™”ì¢‹ë‹¤(2), ìŠ¤í† ë¦¬íƒ„íƒ„(3), ì •ì£¼í–‰(2), ì‹œê°„ìˆœì‚­(3)
â€¢ ê·¹ë‹¨: ê°“ì‘(3), ëª…ì‘(3), ì¸ìƒì›¹íˆ°(3)

ã€ë¶€ì • í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜)ã€‘
â€¢ ê¸°ë³¸: ë³„ë¡œ(1), ë…¸ì¼(3), ì§€ë£¨(2), ìµœì•…(3)
â€¢ ì›¹íˆ°íŠ¹í™”: ì‘í™”ë¶•ê´´(3), ìºë¶•(3), ê¸‰ì „ê°œ(2), ë–¡ë°¥ë°©ì¹˜(3)
â€¢ ê·¹ë‹¨: í•˜ì°¨(3), ì‹œê°„ë‚­ë¹„(3), ë°œì•”(3)
"""
    
    # ë°ì´í„° ê³ ìœ  í‚¤ ìƒì„± (ìºì‹±ìš©)
    data_key = f"{app_name}_{len(df)}"
    
    # ì›¹íˆ° íŠ¹í™” ëª¨ë“œ í† ê¸€
    col1, col2 = st.columns([3, 1])
    with col1:
        st.success(f"âœ… **{len(df):,}ê±´** ë¦¬ë·° ë¶„ì„ ì™„ë£Œ! {f'({app_name})' if app_name else ''}")
    with col2:
        webtoon_mode = st.toggle("ğŸ¨ ì›¹íˆ° íŠ¹í™” ë¶„ì„", value=True, help=webtoon_help)
    
    # ê°ì„± ë¶„ì„: ì´ë¯¸ sentiment ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if "sentiment" not in df.columns:
        # sentiment ì—†ì„ ë•Œë§Œ ë¶„ì„ (ìƒˆë¡œ ìˆ˜ì§‘í•œ ë°ì´í„°)
        cache_key = f"analyzed_{data_key}_{'webtoon' if webtoon_mode else 'basic'}"
        if cache_key in st.session_state:
            df = st.session_state[cache_key]
        else:
            with st.spinner("ğŸ”„ ê°ì„± ë¶„ì„ ì¤‘..."):
                if webtoon_mode:
                    df = analyze_sentiment_webtoon(df)
                else:
                    df = analyze_sentiment_basic(df)
                st.session_state[cache_key] = df
    
    # datetime ë³€í™˜ í™•ì¸
    if not pd.api.types.is_datetime64_any_dtype(df["at"]):
        df["at"] = pd.to_datetime(df["at"])
    
    contents_tuple = tuple(df["content"].tolist())
    
    # íƒ­ êµ¬ì„± (5ê°œ) - ìˆœì„œ: í†µê³„, í† í”½, í‚¤ì›Œë“œ, ìš”ì²­/ë¦¬ë·°, ê°ì„±/ë¶ˆë§Œ
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“ˆ í†µê³„", "ğŸ“‚ í† í”½ë¶„ë¥˜", "ğŸ” í‚¤ì›Œë“œë¶„ì„", "ğŸ™ ìš”ì²­/ë¦¬ë·°", "ğŸ˜Š ê°ì„±/ë¶ˆë§Œ"
    ])
    
    # ----------------------------
    # íƒ­ 1: í†µê³„
    # ----------------------------
    with tab1:
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ì´ ë¦¬ë·°", f"{len(df):,}")
        with col2:
            st.metric("í‰ê·  í‰ì ", f"{df['score'].mean():.1f}â­")
        with col3:
            pos_ratio = (df["sentiment"] == "ê¸ì •").sum() / len(df) * 100
            st.metric("ê¸ì • ë¹„ìœ¨", f"{pos_ratio:.0f}%")
        with col4:
            neg_ratio = (df["sentiment"] == "ë¶€ì •").sum() / len(df) * 100
            st.metric("ë¶€ì • ë¹„ìœ¨", f"{neg_ratio:.0f}%")
        
        st.markdown("---")
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("#### ğŸ—“ï¸ ë‚ ì§œë³„ ë¦¬ë·°")
            daily = df.groupby(df["at"].dt.date).size()
            st.line_chart(daily)
        
        with col2:
            st.markdown("#### â­ í‰ì  ë¶„í¬")
            scores = df["score"].value_counts().sort_index()
            st.bar_chart(scores)
    
    # ----------------------------
    # íƒ­ 5: ê°ì„±/ë¶ˆë§Œ ë¶„ì„ (í†µí•©)
    # ----------------------------
    with tab5:
        # ê°ì„± ë¶„ì„ ì„¹ì…˜
        st.markdown("### ğŸ˜Š ê°ì„± ë¶„ì„")
        
        col1, col2 = st.columns(2)
        
        with col1:
            sentiment_counts = df["sentiment"].value_counts()
            for sentiment, count in sentiment_counts.items():
                pct = count / len(df) * 100
                if sentiment == "ê¸ì •":
                    st.success(f"ğŸ˜Š ê¸ì •: **{count:,}ê±´** ({pct:.1f}%)")
                elif sentiment == "ë¶€ì •":
                    st.error(f"ğŸ˜¤ ë¶€ì •: **{count:,}ê±´** ({pct:.1f}%)")
                else:
                    st.warning(f"ğŸ˜ ì¤‘ë¦½: **{count:,}ê±´** ({pct:.1f}%)")
        
        with col2:
            sentiment_by_score = df.groupby(["score", "sentiment"]).size().unstack(fill_value=0)
            st.dataframe(sentiment_by_score, use_container_width=True)
        
        # ì›¹íˆ° ëª¨ë“œì¼ ë•Œ ê°ì„± ì ìˆ˜ í‘œì‹œ
        if webtoon_mode and "pos_score" in df.columns:
            st.markdown("---")
            st.markdown("#### ğŸ¯ ê°ì„± ì ìˆ˜ ë¶„í¬ (ì›¹íˆ° íŠ¹í™”)")
            col1, col2 = st.columns(2)
            with col1:
                avg_pos = df["pos_score"].mean()
                max_pos = df["pos_score"].max()
                st.metric("í‰ê·  ê¸ì • ì ìˆ˜", f"{avg_pos:.1f}", help=f"ìµœëŒ€ {max_pos}")
            with col2:
                avg_neg = df["neg_score"].mean()
                max_neg = df["neg_score"].max()
                st.metric("í‰ê·  ë¶€ì • ì ìˆ˜", f"{avg_neg:.1f}", help=f"ìµœëŒ€ {max_neg}")
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("#### ğŸ˜Š ê¸ì • í‚¤ì›Œë“œ ì¡°í•©")
            pos_bigrams, _ = analyze_positive_bigram(df)
            if pos_bigrams:
                pos_df = pd.DataFrame(pos_bigrams[:10], columns=["í‚¤ì›Œë“œ ì¡°í•©", "ë¹ˆë„"])
                st.dataframe(pos_df, use_container_width=True, hide_index=True)
        
        with col2:
            st.markdown("#### ğŸ˜¤ ë¶€ì • í‚¤ì›Œë“œ ì¡°í•©")
            neg_bigrams, neg_trigrams, _ = analyze_complaints_trigram(df)
            if neg_bigrams:
                neg_df = pd.DataFrame(neg_bigrams[:10], columns=["í‚¤ì›Œë“œ ì¡°í•©", "ë¹ˆë„"])
                st.dataframe(neg_df, use_container_width=True, hide_index=True)
        
        st.markdown("---")
        
        # ë¶ˆë§Œ ë¶„ì„ ì„¹ì…˜
        st.markdown("### ğŸ˜¤ ë¶ˆë§Œ ì§‘ì¤‘ ë¶„ì„ (1~2ì )")
        
        neg_bigrams, neg_trigrams, neg_df = analyze_complaints_trigram(df)
        
        st.markdown(f"ğŸ”´ ë¶ˆë§Œ ë¦¬ë·°: **{len(neg_df):,}ê±´** ({len(neg_df)/len(df)*100:.1f}%)")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### 2ë‹¨ì–´ ì¡°í•©")
            if neg_bigrams:
                st.dataframe(pd.DataFrame(neg_bigrams[:15], columns=["ì¡°í•©", "ë¹ˆë„"]), use_container_width=True, hide_index=True)
        
        with col2:
            st.markdown("#### 3ë‹¨ì–´ ì¡°í•© (ë§¥ë½)")
            if neg_trigrams:
                st.dataframe(pd.DataFrame(neg_trigrams[:15], columns=["ì¡°í•©", "ë¹ˆë„"]), use_container_width=True, hide_index=True)
        
        # ë¶ˆë§Œ ë¦¬ë·° ì›ë¬¸
        with st.expander(f"ğŸ“‹ ë¶ˆë§Œ ë¦¬ë·° ì›ë¬¸ ({len(neg_df):,}ê±´)", expanded=False):
            search_complaint = st.text_input("ğŸ” ê²€ìƒ‰", key="complaint_search")
            filtered_neg = neg_df.copy()
            if search_complaint:
                filtered_neg = filtered_neg[filtered_neg["content"].str.contains(search_complaint, na=False)]
            
            display_neg = filtered_neg[["at", "score", "content"]].copy()
            display_neg["at"] = display_neg["at"].dt.strftime("%Y-%m-%d")
            display_neg.columns = ["ë‚ ì§œ", "í‰ì ", "ë‚´ìš©"]
            st.dataframe(display_neg, use_container_width=True, hide_index=True, height=300)
    
    # ----------------------------
    # íƒ­ 2: í† í”½ë¶„ë¥˜
    # ----------------------------
    with tab2:
        st.markdown("### ğŸ“‚ í† í”½ë³„ ë¦¬ë·° ë¶„ë¥˜")
        
        topic_data = analyze_topics(contents_tuple)
        sorted_topics = sorted(topic_data.items(), key=lambda x: len(x[1]), reverse=True)
        
        # ìš”ì•½ í…Œì´ë¸”
        summary_data = []
        for topic, reviews_list in sorted_topics:
            summary_data.append({"í† í”½": topic, "ê±´ìˆ˜": len(reviews_list), "ë¹„ìœ¨": f"{len(reviews_list)/len(df)*100:.1f}%"})
        st.dataframe(pd.DataFrame(summary_data), use_container_width=True, hide_index=True)
        
        st.markdown("---")
        
        # í† í”½ë³„ í¼ì¹¨
        for topic, reviews_list in sorted_topics:
            with st.expander(f"{topic} ({len(reviews_list):,}ê±´)", expanded=False):
                if reviews_list:
                    keywords = TOPIC_KEYWORDS[topic]
                    st.caption(f"ğŸ”‘ í‚¤ì›Œë“œ: {', '.join(keywords[:8])}")
                    for i, review in enumerate(reviews_list[:5], 1):
                        truncated = review[:120] + "..." if len(review) > 120 else review
                        st.text(f"{i}. {truncated}")
                else:
                    st.info("í•´ë‹¹ í† í”½ ë¦¬ë·° ì—†ìŒ")
    
    # ----------------------------
    # íƒ­ 3: í‚¤ì›Œë“œ ë¶„ì„ (í†µí•©)
    # ----------------------------
    with tab3:
        # í‚¤ì›Œë“œ ì‹¬ì¸µ ë¶„ì„
        st.markdown("### ğŸ” í‚¤ì›Œë“œ ì‹¬ì¸µ ë¶„ì„")
        st.caption("íŠ¹ì • í‚¤ì›Œë“œ ì…ë ¥ ì‹œ í•´ë‹¹ ë¦¬ë·°ë§Œ ì¶”ì¶œí•˜ì—¬ ë¶„ì„")
        
        # ë¶„ì„í•  í‚¤ì›Œë“œ ì…ë ¥ (íƒ€ì´í‹€ + ì¸í’‹ ê°€ë¡œ ë°°ì¹˜)
        col1, col2 = st.columns([1, 3])
        with col1:
            st.markdown('<p style="margin-top: 8px;">ë¶„ì„í•  í‚¤ì›Œë“œ</p>', unsafe_allow_html=True)
        with col2:
            st.markdown('<div class="keyword-input">', unsafe_allow_html=True)
            deep_keyword = st.text_input("ë¶„ì„í•  í‚¤ì›Œë“œ", value="ì»·ì¸ ", placeholder="ì˜ˆ: ê´‘ê³ , ê²°ì œ", key="deep_kw", max_chars=30, label_visibility="collapsed")
            st.markdown('</div>', unsafe_allow_html=True)
        
        if deep_keyword:
            keyword_df = df[df["content"].str.contains(deep_keyword, na=False, case=False)].copy()
            
            if keyword_df.empty:
                st.warning(f"'{deep_keyword}' í¬í•¨ ë¦¬ë·° ì—†ìŒ")
            else:
                st.success(f"**'{deep_keyword}'** ê´€ë ¨ **{len(keyword_df):,}ê±´** ({len(keyword_df)/len(df)*100:.1f}%)")
                
                col1, col2, col3, col4 = st.columns(4)
                pos_cnt = (keyword_df["sentiment"] == "ê¸ì •").sum()
                neg_cnt = (keyword_df["sentiment"] == "ë¶€ì •").sum()
                
                with col1:
                    st.metric("ë¦¬ë·° ìˆ˜", f"{len(keyword_df):,}")
                with col2:
                    st.metric("í‰ê·  í‰ì ", f"{keyword_df['score'].mean():.1f}â­")
                with col3:
                    st.metric("ê¸ì •", f"{pos_cnt/len(keyword_df)*100:.0f}%")
                with col4:
                    st.metric("ë¶€ì •", f"{neg_cnt/len(keyword_df)*100:.0f}%")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("#### ì—°ê´€ í‚¤ì›Œë“œ")
                    kw_tokens = extract_keywords_cached(tuple(keyword_df["content"].tolist()))
                    kw_tokens = [t for t in kw_tokens if deep_keyword not in t and t not in deep_keyword]
                    kw_counter = Counter(kw_tokens).most_common(10)
                    if kw_counter:
                        st.dataframe(pd.DataFrame(kw_counter, columns=["í‚¤ì›Œë“œ", "ë¹ˆë„"]), use_container_width=True, hide_index=True)
                
                with col2:
                    st.markdown("#### í‚¤ì›Œë“œ ì¡°í•©")
                    bigrams = []
                    for text in keyword_df["content"]:
                        bigrams += extract_bigrams(text)
                    bigrams = [b for b in bigrams if deep_keyword in b]
                    bigram_cnt = Counter(bigrams).most_common(10)
                    if bigram_cnt:
                        st.dataframe(pd.DataFrame(bigram_cnt, columns=["ì¡°í•©", "ë¹ˆë„"]), use_container_width=True, hide_index=True)
                
                # ê¸ì •/ë¶€ì • ë¦¬ë·° ë¹„êµ
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown(f"#### ğŸ˜Š ê¸ì • ({pos_cnt}ê±´)")
                    for _, row in keyword_df[keyword_df["sentiment"] == "ê¸ì •"].head(5).iterrows():
                        st.caption(f"â­{row['score']} | {row['content'][:80]}...")
                with col2:
                    st.markdown(f"#### ğŸ˜¤ ë¶€ì • ({neg_cnt}ê±´)")
                    for _, row in keyword_df[keyword_df["sentiment"] == "ë¶€ì •"].head(5).iterrows():
                        st.caption(f"â­{row['score']} | {row['content'][:80]}...")
                
                st.markdown("---")
                
                # í•˜ìœ„: ê¸ë¶€ì •ë³„ ìµœë‹¤ ë¹ˆë„ í‚¤ì›Œë“œ ë¶„ì„
                st.markdown(f"### ğŸ“Š '{deep_keyword}' ì—°ê´€ ê¸ë¶€ì • í‚¤ì›Œë“œ ë¶„ì„")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("#### ğŸ˜Š ê¸ì • ë¦¬ë·° ìµœë‹¤ í‚¤ì›Œë“œ")
                    pos_keyword_df = keyword_df[keyword_df["sentiment"] == "ê¸ì •"]
                    if not pos_keyword_df.empty:
                        pos_tokens = extract_keywords_cached(tuple(pos_keyword_df["content"].tolist()))
                        pos_tokens = [t for t in pos_tokens if deep_keyword not in t and t not in deep_keyword]
                        pos_kw_counter = Counter(pos_tokens).most_common(15)
                        if pos_kw_counter:
                            st.dataframe(pd.DataFrame(pos_kw_counter, columns=["í‚¤ì›Œë“œ", "ë¹ˆë„"]), use_container_width=True, hide_index=True)
                            
                            # ì›Œë“œí´ë¼ìš°ë“œ
                            img_bytes = generate_wordcloud_image(tuple(pos_kw_counter), FONT_PATH)
                            if img_bytes:
                                st.image(img_bytes, use_container_width=True)
                    else:
                        st.info("ê¸ì • ë¦¬ë·° ì—†ìŒ")
                
                with col2:
                    st.markdown("#### ğŸ˜¤ ë¶€ì • ë¦¬ë·° ìµœë‹¤ í‚¤ì›Œë“œ")
                    neg_keyword_df = keyword_df[keyword_df["sentiment"] == "ë¶€ì •"]
                    if not neg_keyword_df.empty:
                        neg_tokens = extract_keywords_cached(tuple(neg_keyword_df["content"].tolist()))
                        neg_tokens = [t for t in neg_tokens if deep_keyword not in t and t not in deep_keyword]
                        neg_kw_counter = Counter(neg_tokens).most_common(15)
                        if neg_kw_counter:
                            st.dataframe(pd.DataFrame(neg_kw_counter, columns=["í‚¤ì›Œë“œ", "ë¹ˆë„"]), use_container_width=True, hide_index=True)
                            
                            # ì›Œë“œí´ë¼ìš°ë“œ
                            img_bytes = generate_wordcloud_image(tuple(neg_kw_counter), FONT_PATH)
                            if img_bytes:
                                st.image(img_bytes, use_container_width=True)
                    else:
                        st.info("ë¶€ì • ë¦¬ë·° ì—†ìŒ")
        
        else:
            st.caption("ğŸ’¡ ì¶”ì²œ: ê´‘ê³ , ê²°ì œ, ë²„ê·¸, ë¡œë”©, ì‘í’ˆ, ì—°ì¬, ì¿ í‚¤")
    
    # ----------------------------
    # íƒ­ 4: ìš”ì²­/ë¦¬ë·° (í†µí•©)
    # ----------------------------
    with tab4:
        # ìš”ì²­ì‚¬í•­ ì„¹ì…˜
        st.markdown("### ğŸ™ ì‚¬ìš©ì ìš”ì²­ì‚¬í•­")
        
        requests = extract_requests(contents_tuple)
        
        if requests:
            col1, col2 = st.columns(2)
            with col1:
                st.markdown(f"#### ìš”ì²­ì‚¬í•­ TOP 15")
                st.dataframe(pd.DataFrame(requests[:15], columns=["ìš”ì²­", "íšŸìˆ˜"]), use_container_width=True, hide_index=True)
            with col2:
                st.markdown("#### ìš”ì²­ ë¹ˆë„")
                st.bar_chart(pd.DataFrame(requests[:8], columns=["ìš”ì²­", "íšŸìˆ˜"]).set_index("ìš”ì²­"))
        else:
            st.info("ìš”ì²­ì‚¬í•­ ì—†ìŒ")
        
        st.markdown("---")
        
        # ë¦¬ë·° ì›ë¬¸ ì„¹ì…˜
        st.markdown("### ğŸ“ ë¦¬ë·° ì›ë¬¸")
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰, í‰ì , ê°ì„± ê°™ì€ ë¼ì¸
        col1, col2, col3 = st.columns(3)
        with col1:
            st.markdown('<div class="keyword-input">', unsafe_allow_html=True)
            keyword = st.text_input("í‚¤ì›Œë“œ ê²€ìƒ‰", key="review_search", max_chars=30, placeholder="ê²€ìƒ‰ì–´ ì…ë ¥")
            st.markdown('</div>', unsafe_allow_html=True)
        with col2:
            score_filter = st.multiselect("í‰ì ", [1,2,3,4,5], default=[1,2,3,4,5], key="review_score")
        with col3:
            sentiment_filter = st.multiselect("ê°ì„±", ["ê¸ì •", "ì¤‘ë¦½", "ë¶€ì •"], default=["ê¸ì •", "ì¤‘ë¦½", "ë¶€ì •"], key="review_sent")
        
        filtered = df[df["score"].isin(score_filter) & df["sentiment"].isin(sentiment_filter)]
        if keyword:
            filtered = filtered[filtered["content"].str.contains(keyword, na=False)]
        
        st.write(f"**{len(filtered):,}ê±´**")
        
        display_df = filtered[["at", "score", "sentiment", "content"]].copy()
        display_df["at"] = display_df["at"].dt.strftime("%Y-%m-%d")
        display_df.columns = ["ë‚ ì§œ", "í‰ì ", "ê°ì„±", "ë‚´ìš©"]
        st.dataframe(display_df, use_container_width=True, hide_index=True, height=400)

# ----------------------------
# ë©”ì¸ UI
# ----------------------------
st.markdown("#### ğŸ“Š ì•± ë¦¬ë·° ë¶„ì„ &nbsp;&nbsp;|&nbsp;&nbsp; [GitHub](https://github.com/blendiing/appread)")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.markdown("#### ğŸ” ì•± ID")
    app_id_input = st.text_input(
        "ì•± ID",
        value="",
        placeholder="com.example.app",
        label_visibility="collapsed"
    )
    
    # ìƒ˜í”Œ ì•± ID - ì»´íŒ©íŠ¸í•˜ê²Œ
    with st.expander("ğŸ“‹ ìƒ˜í”Œ ì•± ID", expanded=False):
        st.code("com.nhn.android.webtoon")
        st.caption("ë„¤ì´ë²„ ì›¹íˆ°")
        st.code("com.kakaopage.app")
        st.caption("ì¹´ì¹´ì˜¤í˜ì´ì§€")
        st.code("com.initialcoms.ridi")
        st.caption("ë¦¬ë””ë¶ìŠ¤")
    
    st.markdown("---")
    
    # ìˆ˜ì§‘ ì˜µì…˜
    review_count = st.select_slider(
        "ğŸ“Š ìˆ˜ì§‘ ë¦¬ë·° ìˆ˜",
        options=[100, 300, 500, 700, 1000],
        value=500
    )
    
    # ë°ì´í„° ìˆ˜ì§‘ ë²„íŠ¼
    has_input = app_id_input is not None and len(app_id_input.strip()) > 0
    collect_btn = st.button(
        "ğŸš€ ìˆ˜ì§‘ ì‹œì‘", 
        type="primary", 
        use_container_width=True,
        disabled=(not has_input)
    )
    
    if not has_input:
        st.caption("ğŸ’¡ ì•± ID ì…ë ¥ ì‹œ í™œì„±í™”")

# ë©”ì¸ ì½˜í…ì¸ 
# ìˆ˜ì§‘ ë²„íŠ¼ í´ë¦­ ì‹œ ë°ì´í„° ìˆ˜ì§‘
if collect_btn and has_input:
    with st.spinner(f"ğŸ“¥ {app_id_input} ë¦¬ë·° ìˆ˜ì§‘ ì¤‘... ({review_count}ê±´)"):
        df = get_reviews_cached(app_id_input, count=review_count)
        df = df.sort_values(by="at", ascending=False)
        st.session_state["collected_df"] = df
        st.session_state["collected_app"] = app_id_input

# ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í‘œì‹œ
if st.session_state.get("collected_df") is not None and not st.session_state["collected_df"].empty:
    display_analysis(st.session_state["collected_df"], st.session_state.get("collected_app", ""))

# ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°ì´í„° í‘œì‹œ (load_default_dataê°€ ì´ë¯¸ ë¶„ì„ ì™„ë£Œ)
else:
    default_df = load_default_data()  # @st.cache_dataë¡œ ìºì‹±ë¨, ê°ì„±ë¶„ì„ í¬í•¨
    display_analysis(default_df, "ë„¤ì´ë²„ ì›¹íˆ°", "ğŸ“Œ **ê¸°ë³¸ ë°ì´í„°**: ë„¤ì´ë²„ ì›¹íˆ° ë¦¬ë·° 1,000ê±´ (2025.01.19 ê¸°ì¤€)")

st.markdown("---")
st.caption("Made with â¤ï¸ using Streamlit | ë°ì´í„°: Google Play Store")
